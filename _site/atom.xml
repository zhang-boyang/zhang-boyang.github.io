<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title>Note of John Brown</title>
  <updated>2019-09-28T17:34:54+08:00</updated>
  <id></id>
  <author>
    <name>John Brown</name>
    <email></email>
  </author>

  
  <entry>
    <title>分布式系统分片的艺术(2017-6.824 Lab4)</title>
    <link href="http://localhost:4000//2019/09/28/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%88%86%E7%89%87%E7%9A%84%E8%89%BA%E6%9C%AF(2017-6.824-Lab4).html"/>
    <updated>2019-09-28T00:00:00+08:00</updated>
    <id>//2019/09/28/分布式系统分片的艺术(2017-6.824 Lab4)</id>
    <content type="html">&lt;h2 id=&quot;id-为什么要分片&quot;&gt;为什么要”分片”&lt;/h2&gt;

&lt;p&gt;分片即将整体的数据分而治之。以多节点系统工作的方式共同完成一个工作。能够减轻节点服务器压力，提高整个集群节点利用率与性能。&lt;/p&gt;

&lt;p&gt;这里引申出一个问题，为什么需要使用分片这一方式。在分布式系统中，有主副节点(master replica 数据上，Leader Follower raft一致性协议上)。能否在主副节点同时对数据进行操作，哪怕只是对副本进行读操作。至少在要求&lt;strong&gt;强一致&lt;/strong&gt;或者&lt;strong&gt;线性一致的&lt;/strong&gt;的存储系统中，答案是否定的。在这些系统中，副本只负责提高分布式系统的可用性，对系统的容灾性进行提升。client不能使用replica中的数据。因为副本节点没有向一致性propose的权利，无法保证这次操作已达成了共识。最主要的原因就是如果集群出现了脑裂,导致线性不一致。&lt;/p&gt;

&lt;p&gt;这里需要提到一点，就是原始的paxos协议，没有raft的leader和follower的区别。paxos角色为proposer和acceptor，但是任何节点都同时扮演这两种角色，一个基础的paxos协议需要多次网络请求（prepare，accept…）。任何节点都可以进行propose的结果就是导致了非常容易发生冲突，在多请求时很很难达成共识。所以一般在工程实现时使用multi-paxos，即也是选出leader打头阵。&lt;/p&gt;

&lt;h2 id=&quot;id-怎样进行分片&quot;&gt;怎样进行分片&lt;/h2&gt;

&lt;p&gt;首先分片如下图所示&lt;img src=&quot;/images/raftblog/shard_layer.png&quot; alt=&quot;=&quot; /&gt;：一个分片属于一个组，一个组可以包含多个分片。一个副本组组成一个raft集群，多台服务器(&lt;script type=&quot;math/tex&quot;&gt;n\ge3&lt;/script&gt;)属于一个副本组，一台服务器同时归属于多个副本组。分片数据被一个副本组保管，以保证可用性。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;分片的个数&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首先一旦分片个数确定，在系统运行中就成为定值，不可变更。需要将整个数据平均分成几片？具体情况具体分析。主要根据实际的节点数量而定。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;怎样分片&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;需要建立一个ShardMaster服务告诉客户端与服务端目前数据与集群的情况，我们把它成为配置。配置对应一个序号，每次配置变更时，其所对应的序号自增。最开始的配置编号为0。这个配置不包括任何组，所有分片都在GID0（即无效GID）中。下一个配置为1，以此类推。&lt;/p&gt;

&lt;p&gt;ShardMaster也是一个raft集群，保证其一致性与可用性。&lt;/p&gt;

&lt;p&gt;ShardMaster的工作就是管理配置。每一份配置由一个副本组集合和每个副本组中分配的分片组成。当配置需要被改变时，分片master就会根据现有情况重新创建一个新的配置。当k/v的客户端和服务端需要读取当前（或是以前的配置）时，就会与分片shard进行通讯。这里需要如下接口：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Join&lt;/code&gt;RPC增加一个新的副本组。他的参数是使用一个唯一的非零的GID对应服务器名称。分片master应当生成一个包含新副本组的新配置。新配置应当尽可能的将分片分的均匀，而且还要尽可能的减少分片在副本组之间的移动。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Leave&lt;/code&gt;RPC的参数是一个之前加入要离开的组GID的list，分片master应当重新变动配置将这些组从配置中剔除，并且将分片重新分配给剩余的组中。新的配置同样应当尽可能的将分片分的均匀，而且还要尽可能的减少分片在副本组之间的移动。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Query&lt;/code&gt;RPC的参数是一个配置号。分片master返回这个号码对应的配置。如果这个号码是-1或者比任意一个已知的号码都大，那么分片master应当返回最新的配置。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;怎样保证分组的均匀&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;何为均匀，&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
N即拥有分片数量最多的组的分片数 - N拥有数量最少的组的分片数 &lt;2 %]]&gt;&lt;/script&gt;。
   最基本的有两种方法：第一种就是分布式一致性哈希，这种方法有一个有点就是逻辑简单，但是不能保证分片非常的均匀，需要最后进行在平均。同时这个算法不能保证分片在副本间移动的尽可能的少。第二种方法就是均分法。将现有组多处的分片均匀的分给新加入的组中，同样将离去组的分片平均的分摊到剩余组中，就是实现起来比较绕。&lt;/p&gt;

&lt;h2 id=&quot;id-分片kv服务&quot;&gt;分片kv服务&lt;/h2&gt;

&lt;p&gt;每一个分片kv服务作为一个副本组的一部分，每一个副本组为以key为纬度的部分分片提供Get Put和Append操作服务。客户端使用key2shard()函数查找自己的key属于哪一个分片。过个副本组共同组成了一个分片集合。shardmaster实例为副本组分配分片。当分配改变时，副本组根据配置把不再对应的分片交接给其他的分本组，同时保证客户侧数据的完整性。每一个分片需要在raft副本组中的多数副本可以与彼此通信并且与多数shardmaster能够通信即可。也就是说，你的服务即使在少数要在少数的副本组宕机，暂时不可用等情况下依然保持服务。&lt;/p&gt;

&lt;p&gt;这里要思考一个问题：对于一个副本组而言，当配置改变时，不再归属自己的分片怎样处理，自己需要的分片又改怎样获取？&lt;/p&gt;

&lt;p&gt;还有以下场景：
设，Group1 包含shard1，shard2，我们把它计作 &lt;script type=&quot;math/tex&quot;&gt;G_1:\{S_1,S_2\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;T_1&lt;/script&gt;时刻集群配置为：&lt;script type=&quot;math/tex&quot;&gt;G_1:\{S_1,S_2\}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;G_2:\{S_3,S_4\}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;G_3:\{S_5\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;T_2=&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;G_1:\{S_1,S_2,S_5\}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;G_2:\{S_3,S_4\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;T_3=&lt;/script&gt; &lt;script type=&quot;math/tex&quot;&gt;G_1:\{S_1,S_5\}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;G_2:\{S_3\}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;G_4\{S_2,S_4\}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt; 就会遇到既要向&lt;script type=&quot;math/tex&quot;&gt;G_3&lt;/script&gt;获取&lt;script type=&quot;math/tex&quot;&gt;S_5&lt;/script&gt;和向&lt;script type=&quot;math/tex&quot;&gt;G_4&lt;/script&gt;提供&lt;script type=&quot;math/tex&quot;&gt;S_2&lt;/script&gt;的情况。&lt;/p&gt;

&lt;p&gt;在满足基本服务时还需要考虑两种情况：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;垃圾回收&lt;/p&gt;

    &lt;p&gt;当副本对某个分片失去所有权后，副本需要将这个分片的key从数据库中删除掉。将其存储并保存不再请求的键值是非常浪费资源的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在配置变更时处理客户端的请求&lt;/p&gt;

    &lt;p&gt;比如&lt;script type=&quot;math/tex&quot;&gt;G_3&lt;/script&gt;在某次配置变更时，需要&lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt;的&lt;script type=&quot;math/tex&quot;&gt;S_1&lt;/script&gt;和&lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt;的&lt;script type=&quot;math/tex&quot;&gt;S_2&lt;/script&gt;，那么我们需要&lt;script type=&quot;math/tex&quot;&gt;G_3&lt;/script&gt;在获取到其中的分片后立即开始工作，即使还有某些分片没有到位。比如，&lt;script type=&quot;math/tex&quot;&gt;G_1&lt;/script&gt;突然宕机，但是&lt;script type=&quot;math/tex&quot;&gt;G_3&lt;/script&gt;在获取到&lt;script type=&quot;math/tex&quot;&gt;G_2&lt;/script&gt;提供的&lt;script type=&quot;math/tex&quot;&gt;S_2&lt;/script&gt;后就开始工作，不管这个配置的变更最终是否全部完成。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;为了处理这些问题，需要在副本组中将保有这些的分片都标记状态：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;RECV&lt;/code&gt;: 表示此分片正在向其他副本组获取中，不能提供服务，不能被GC&lt;/p&gt;

&lt;p&gt;&lt;code&gt;WORK&lt;/code&gt;: 表示此分片属于本副本组，可以对外提供服务，不能被GC&lt;/p&gt;

&lt;p&gt;&lt;code&gt;TRANS&lt;/code&gt;: 表示此分片目前归属与其他分本组，正在向其他副本组传输，不可对外提供服务，不能被GC&lt;/p&gt;

&lt;p&gt;&lt;code&gt;REMV&lt;/code&gt;: 表示已没有在使用，不可对外提供服务，可以被GC&lt;/p&gt;

&lt;p&gt;他们之间只能是以下状态的状态转移方式&lt;code&gt;RECV&lt;/code&gt; -&amp;gt;&lt;code&gt;WORK&lt;/code&gt;-&amp;gt;&lt;code&gt;TRANS&lt;/code&gt;-&amp;gt;&lt;code&gt;REMV&lt;/code&gt;,&lt;code&gt;REMV&lt;/code&gt;-&amp;gt;&lt;code&gt;RECV&lt;/code&gt;&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Raft的go语言实现与使用(2017-6.824 Lab2-Lab3)</title>
    <link href="http://localhost:4000//2019/09/28/Raft%E7%9A%84go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BD%BF%E7%94%A8(2017-6.824-Lab2-Lab3).html"/>
    <updated>2019-09-28T00:00:00+08:00</updated>
    <id>//2019/09/28/Raft的go语言实现与使用(2017-6.824 Lab2-Lab3)</id>
    <content type="html">&lt;p&gt;Raft协议的解读已经有很多，整体最主要的协议其实用论文中的两幅图就能大致表述了。最后再加上一个Snapshot的图解析基本就全。相对与Paxos而言，我个人认为Raft从实现协议的角度来说反而比Paxos要复杂。主要就是在于index的处理上。这此使用Go语言实现了一个Raft协议并支持snapshot。并且使用Raft协议实现了一个分布式的kv存储。&lt;/p&gt;

&lt;h2 id=&quot;id-raft的实现&quot;&gt;Raft的实现&lt;/h2&gt;

&lt;h3 id=&quot;id-raft的成员变量&quot;&gt;Raft的成员变量&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;    type Raft struct {
        mu        sync.Mutex          // 锁
        peers     []*labrpc.ClientEnd // 所用成员的通讯信息
        persister *Persister          // 持久化信息
        me        int                 // 本节点在成员通讯信息的idx
        //Persistent state on all servers
        CurrentTerm int        // 目前的Term，初始值为0
        VotedFor    int        // 当前我投票给了谁
        Log         []LogEntry //log信息
        // Volatile state
        CommitIndex int // 目前确定提交的log的idx
        lastApplied int // index of highest log entry applied to state machine init 0
        // Only for leader
        nextIndex     map[int]int //for each server, index of the next long entry to that server
        matchIndex    map[int]int // for each server, index of highest long entry known to be replicated on server
        ChanCharacter chan int          //节点改变时的通知chan
        Character     int               //当前本节点在Raft中的角色
        TimeRest      *SaftyTimeOut     //定时器
        ApplyMsgChan  chan ApplyMsg     //传输应用的信息的chan

        PrevSnapIndex int               //有快照时，快照保存的最近的idx
        PrevSnapTerm  int               //有快照时，快照保存的最近的idx的Term
    }
    
    //与外界传输信息的结构
type ApplyMsg struct {
	Index       int
	Command     interface{}
	UseSnapshot bool
	Snapshot    []byte
}

//log中包含的信息  Command中包含着需要达成一致的东西
type LogEntry struct {
	Term    int
	Index   int
	Command interface{}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;id-首先raft对外提供的接口&quot;&gt;首先Raft对外提供的接口&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;对外接口&lt;/p&gt;

    &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt; func (rf *Raft) GetState() (int, bool) {/*...*/}
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;返回当前的Term和这个服务器是不是Leader（自己认为自己是否是Leader）&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt; func (rf *Raft) Start(command interface{}) (int, int, bool){/*...*/}
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;输入参数为需要达成一致的日志内容，返回日志的index，当前term和是否是leader（自己认为的）。&lt;/p&gt;

    &lt;p&gt;&lt;code&gt;applyCh chan ApplyMsg&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;一个对外提供已达成协议的管道。如果某项日志在成员之中通过了，那么就通过此通道告诉使用方。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对内的服务接口&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt; func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply){/*...*/}
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;接口&lt;code&gt;AppendEntries&lt;/code&gt;主要作用是Leader向Follower添加Entry到Follower的日志当中，或者发送心跳包。在返回的reply中，Follower需要返回的当前的Term，出否成功等信息。&lt;/p&gt;

    &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;     type AppendEntriesArgs struct {
         Term         int        //current Term
         LeaderID     int        // LeaderID
         PreLogIndex  int        //index of log entry immediately preceding new ones
         PreLogTerm   int        //term of prevLogIndex entry
         LogEntries   []LogEntry //log entries to store, request is heart beat if it's empty
         LeaderCommit int        //leader’s CommitIndex
     }

     type AppendEntriesReply struct {
         Term          int  //currentTerm, for leader to update itself
         Success       bool //return true if follower contained entry matching prevLogIndex and prevLogTerm
         isOk          bool // rpc is ok
         ConflictIndex int
         ConflictTerm  int
     }
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;发送方(Leader)：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;如果(&lt;code&gt;nextIndex[FollowerID]&lt;/code&gt;已经没有新的日志需要传递了，args.LogEntries 置为空，否则传送从index从&lt;code&gt;nextIndex[FollowerID]&lt;/code&gt; 到自己最近的Log)&lt;/li&gt;
      &lt;li&gt;LeaderCommit置为目前已经达成一致的日志的Index(commitIndex)。&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;接收方(Follower)：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;如果当前currentTerm &amp;gt; Term, 返回false。如果currentTerm &amp;lt; Term，将自己的currentTerm置为Term&lt;/li&gt;
      &lt;li&gt;刷新定时器&lt;/li&gt;
      &lt;li&gt;如果日志中preLogIndex的Term与PreLogTerm不一样，Success为false(&lt;code&gt;if Log[preLogIndex].Term == PreLogTerm&lt;/code&gt;)&lt;/li&gt;
      &lt;li&gt;如果已存在的日志与新来的这条冲突（相同的index不同的term），删除现有的entry,按照leader发送过来为准&lt;/li&gt;
      &lt;li&gt;将所有新的日志项都追加到自己的日志中&lt;/li&gt;
      &lt;li&gt;LeaderCommit &amp;gt; commitIndex,将commitIndex = min(leaderCommit, 最新日志项index)&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;发送方(Leader)：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;如果返回的Term &amp;gt; currentTerm, 将自己的角色置为Follower， currentTerm置为Term。不在处理下面的流程。&lt;/li&gt;
      &lt;li&gt;如果Success为false，将这个Follower的nextIndex进行减一。如果为true，那么就把这个Follower的nextIndex值加(&lt;code&gt;nextIndex[FollowerID] += len(args.LogEntries)&lt;/code&gt;)&lt;/li&gt;
      &lt;li&gt;达成一致后选取达成一致(quorum)的index置为commitIndex&lt;/li&gt;
    &lt;/ul&gt;

    &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt; func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply){/*...*/}
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;接口&lt;code&gt;RequestVote&lt;/code&gt;为Candidate向其他成员发起投票时当选Leader的接口。&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt; type RequestVoteArgs struct {
     Term         int // candidate's term
     CandidateID  int //candidate requesting vote
     LastLogIndex int //index of candidate
     LastLogTerm  int //term of candidate's last log entry
 }

 type RequestVoteReply struct {
     Term        int  // currentTerm, for candidate to update itself
     VoteGranted bool // true means candidate received vote
     ok          bool // rpc is ok
 }
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;发送方(Candidate)：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;增加currentTerm&lt;/li&gt;
      &lt;li&gt;自己先投自己一票&lt;/li&gt;
      &lt;li&gt;重置计时器&lt;/li&gt;
      &lt;li&gt;向其他服务器发送RequestVote&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;接收方:&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;如果args.Term &amp;lt; currentTerm, 返回false&lt;/li&gt;
      &lt;li&gt;如果args.Term &amp;gt; currentTerm, currentTerm = args.Term，改变角色为Follower。如果&lt;/li&gt;
      &lt;li&gt;如果votedFor为空或者有candidateID，并且候选人的日志至少与接收者的日志一样新，投赞成票并刷新计时器。至少一样新是指：&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;code&gt;args.LastLogIndex &amp;gt; my.LastLogIndex || (args.LastLogIndex == my.LastLogIndex &amp;amp;&amp;amp; LastLogTerm &amp;gt;= Log[LastLogIndex].Term)&lt;/code&gt;&lt;/p&gt;

    &lt;p&gt;发送方(Candidate)：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;如果收到的投票中term大于curTerm，curTerm = term，转变为Follower&lt;/li&gt;
      &lt;li&gt;如果投票RPC收到了来自多数服务器的票，当选leader。&lt;/li&gt;
      &lt;li&gt;如果收到了来自新Leader的AppendEntries RPC（term不小于curTerm），转变为follower&lt;/li&gt;
      &lt;li&gt;如果选举超时，开始新一轮的选举&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;id-raft-日志的快速收敛&quot;&gt;Raft 日志的快速收敛&lt;/h3&gt;
&lt;p&gt;在工程实现中，会出现一种情况。就是当网络分裂时，形成了两个区域，一个含有新leader的区域，一个为老leader但不能达成法定人数的区域。如果持续有客户端项老leader的区域发送请求，虽然这些请求并不会达成一致性，但是，在老区域中的节点日志中会积累很多这样的未达成一致性的协议。如果网络愈合后，新leader在向老集群同步新日志时，如果一个一个使用递减index的方式区试时，会发现这种情况下会花费很长时间都不能找到合法的PreLogIndex。所以需要使用快速收敛，使其能够更快个同步日志。
   &lt;img src=&quot;/images/raftblog/fast_roll_back.png&quot; alt=&quot;=&quot; /&gt;。&lt;/p&gt;

&lt;p&gt;在论文中提到的方式是：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;接收方在PreLogIndex不一致时，返回目前这个PreLogIndex这条日志的Term（ConflictTerm），并返回自己日志中这个第一次出现这个Term的index（ConflictIndex）。&lt;/li&gt;
  &lt;li&gt;发送方在自己的日志中找到这一个ConflictIndex，如果这条日志的Term与ConflictTerm相同，找到这个Term最后一个index（或者直接根据ConflictTerm寻找）。将这个–index作为下一次这个接收方PreLogIndex的传参。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个方法主要就是利用了Quorum（抽屉原理），如果有新集群能够正常工作，那么其他非必然无法commit新日志。(我这里有一个想法：Old Peer可以返回自己最新的已commit的index，虽然可能会存在自己没有commit但是新集群已经被commit的日志，但是这些日志应该不会太多，这样的话收敛协议比较简单)&lt;/p&gt;

&lt;h3 id=&quot;id-raft-一致性安全&quot;&gt;Raft 一致性安全&lt;/h3&gt;

&lt;p&gt;Raft的安全性如论文中x图所示，在leader频繁更换时，即使Leader成功发送给Quorum数量的某个提议，但是在没有收到Quorum数量的commit的情况下。不能够告诉客户端这条日志已经被确认了。Raft作者的博士论文的图更清晰明确一些&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/raftblog/paper_safty.png&quot; alt=&quot;=&quot; /&gt;。&lt;/p&gt;

&lt;p&gt;所以正确的确认方式应当遵循下图的逻辑&lt;img src=&quot;/images/raftblog/raft_commit.png&quot; alt=&quot;=&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;id-raft-快照&quot;&gt;Raft 快照&lt;/h3&gt;

&lt;p&gt;快照就是当raft的日志很多时，造成空间浪费，因为之前很早已经达成一致性的日志已经没有必要存在了。这时需要使用快照的策略对用户的状态进行保存，并且抛弃已经给客户达成一致性的日志。&lt;img src=&quot;/images/raftblog/snapshot.png&quot; alt=&quot;=&quot; /&gt;&lt;/p&gt;

&lt;p&gt;是否生成快照是由用户发起的，用户查看rf中log的大小，在超过指定的阈值后，每一个用户的阈值可以不同，那么他在何时需要生成快照也不同。快照唯一需要Leader操心的就是：当一个新raft成员加入到集群中，或者一个比较旧的raft成员重新加入到集群时，由于“欠账”太多时（Leader已经没有当时那个index的信息），需要Leader强行同步快照。&lt;/p&gt;

&lt;p&gt;接口：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;    func (rf *Raft) ReadyToMakeSnapShot(index int) {/*...*/}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个接口的主要目开始制作传入从这个index开始制作raft日志（这个index之后全部抛弃）&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;type InstallSnapShotReq struct {
    Term              int
    Leader            int       
    LastIncludedIndex int       //快照中包含的最大Index
    LastIncludedTerm  int       //快照中包含的最大Term
    Offset            int       //唯一（多次传输使用）
    Data              []byte    //数据
    Done              bool      //是否已完成（多次传输使用）
    SnapshotLog       LogEntry  //需要
}

type InstallSnapShotRsp struct {
    CurTerm int     //当前term
}

func (rf *Raft) InstallSnapShot(args *InstallSnapShotReq, rsp *InstallSnapShotRsp) {/*...*/}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;InstallSnapShot&lt;/code&gt; 这一接口的目的就是Leader发送给“欠账太多”的Follower。让这个“落后”的Follower更快的”catch up”。
接受者：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;如果args.Term &amp;gt; curTerm 立即返回rsp.CurTerm&lt;/li&gt;
  &lt;li&gt;如果自己不是Follower，变成为Follower&lt;/li&gt;
  &lt;li&gt;重制定时器&lt;/li&gt;
  &lt;li&gt;修改自己的PrevSnapIndex，PrevSnapTerm，CommitIndex&lt;/li&gt;
  &lt;li&gt;生成一个Msg，里边包括args.data 并在Msg中做一个UseSnapshot为True的标记&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;返回：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;如果rsp.CurTerm &amp;gt; args.Term, 由Leader变为Follower，&lt;/li&gt;
  &lt;li&gt;否则将这个Follower的Next变为arg.LastIncludedIndex&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;id-使用raft协议的分布式的kv存储&quot;&gt;使用Raft协议的分布式的KV存储&lt;/h2&gt;

&lt;p&gt;使用Raft做一个分布式KV存储(在这里称做kvraft)主要是为了更好的了解Raft的一些特性，即把Raft用起来。KV只要能保证是Raft的Quorum容灾和线性一致就可以。所以KV存储向外提供三个接口：&lt;code&gt;Put(key, value)&lt;/code&gt;, &lt;code&gt;Append(key, arg)&lt;/code&gt;, 和 &lt;code&gt;Get(key)&lt;/code&gt;。服务为一个基本的KV数据库存储。Put将一个特定的Key放入数据库中，Append将arg中的值追加到键值中，Get获取当前key的值。如果遇到Append一个不存在的key，相当于Put操作。&lt;/p&gt;

&lt;p&gt;具体的kvraft的逻辑如下图所示：
&lt;img src=&quot;/images/raftblog/kvraft_logic.png&quot; alt=&quot;=&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;id-用户侧clerk&quot;&gt;用户侧(Clerk)&lt;/h3&gt;
&lt;p&gt;首先用户侧使用Clerk进行操作，Clerk包含了三个接口&lt;code&gt;Put(key, value)&lt;/code&gt;, &lt;code&gt;Append(key, arg)&lt;/code&gt;, 和 &lt;code&gt;Get(key)&lt;/code&gt;，暴露给用户，用户使用类似与&lt;code&gt;Clerk.Put(k,v)&lt;/code&gt;进行操作。之所以不直接调用KVraft服务的是因为Clerk做了以下操作：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;为了防止重复请求，每一次请求需要有一个全局唯一且递增的序列号&lt;/li&gt;
  &lt;li&gt;由于序列号的原因，在没有请求服务成功之前是不会返回的&lt;/li&gt;
  &lt;li&gt;需要查找KVraft服务到底那台机器是Raft的Leader&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code class=&quot;language-go&quot;&gt;func (ck *Clerk) OP(key string) string {
	// You will have to modify this function.
	var args GetArgs
	var value string
	args.Key = key
	args.ID = ck.ID
	ck.mu.Lock()
	ck.Seq++
	args.Seq = ck.Seq
    ck.mu.Unlock()
    var reply GetReply
	for {
		ok := ck.servers[ck.leaderNo].Call(&quot;RaftKV.OP&quot;, &amp;amp;args, &amp;amp;reply)
		if ok &amp;amp;&amp;amp; !reply.WrongLeader &amp;amp;&amp;amp; reply.Err != KVTimeOut {
			if reply.Err == OK{
                break
            }
		}
		//something worry with network or leader change. retry other server
		ck.mu.Lock()
		ck.leaderNo = (ck.leaderNo + 1) % len(ck.servers)
		ck.mu.Unlock()
	}
	return reply.value
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;id-服务侧kvraft&quot;&gt;服务侧(kvraft)&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;服务侧对外提供三个接口&lt;code&gt;Put(key, value)&lt;/code&gt;, &lt;code&gt;Append(key, arg)&lt;/code&gt;, 和 &lt;code&gt;Get(key)&lt;/code&gt;。节点与节点之间使用通讯完全使用Raft协议进行“交流”(数据传输)。&lt;/li&gt;
  &lt;li&gt;服务器与每一个请求都需要经过Raft一致性协议的协商，包括Get请求。因为会出现如下状况&lt;img src=&quot;/images/raftblog/net_partition.png&quot; alt=&quot;=&quot; /&gt;:当旧Leader被网络隔离时，结果会出现线性不一致。&lt;/li&gt;
  &lt;li&gt;如果出现网络错误可能会造成重复提交，比如一个写操作，服务端已经写入，但是在返回给用户时出现了网络失败，用户无法收到写成功的返回，会重复这个操作，造成数据错误。所以服务器需要记录用户ID以及对应这个用户的最大请求Seq，以保证重复请求不会影响服务。&lt;/li&gt;
  &lt;li&gt;Raft的Msg分为两种，一种为普通请求，一种为Raft的安装快照。&lt;/li&gt;
&lt;/ol&gt;
</content>
  </entry>
  
  <entry>
    <title>关于ABtest哈希算法正交性讨论</title>
    <link href="http://localhost:4000//2019/01/29/%E5%85%B3%E4%BA%8EABtest%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%E6%AD%A3%E4%BA%A4%E6%80%A7%E8%AE%A8%E8%AE%BA.html"/>
    <updated>2019-01-29T00:00:00+08:00</updated>
    <id>//2019/01/29/关于ABtest哈希算法正交性讨论</id>
    <content type="html">&lt;h1 id=&quot;id-关于abtest哈希算法正交性讨论&quot;&gt;关于ABtest哈希算法正交性讨论&lt;/h1&gt;

&lt;h2 id=&quot;id-hash正交性&quot;&gt;Hash正交性&lt;/h2&gt;
&lt;p&gt;正交性定义：首先任意一个hash_type都可以均匀的将流量均匀分成100份。使用hash_type1 进行哈希后得到&lt;script type=&quot;math/tex&quot;&gt;x_i,i\in[0,100)&lt;/script&gt;,即每份百分之1的流量，再使用hash_type2进行哈希后得到&lt;script type=&quot;math/tex&quot;&gt;y_i,i\in[0,100)&lt;/script&gt;。如果某个key在被hash_type1进行hash后得到的流量与被hash_type2得到的流量是无关的，那么就说明hash_type1与hash_type2是两中hash方式是正交的。&lt;/p&gt;

&lt;p&gt;例如一个1000000用户的流量，经过hash_type1与hash_type2进行hash，理想情况下
中具有相同每份y中的相同x流量的个数也是同样多的，即100个。如果使用数组命中计数的话，list[&lt;script type=&quot;math/tex&quot;&gt;x*100 + y&lt;/script&gt;], list[&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;] = 100。越不均匀，也就代表这越相关。&lt;/p&gt;

&lt;h2 id=&quot;id-bkdrhash算法&quot;&gt;BKDRhash算法&lt;/h2&gt;
&lt;p&gt;BKDRhash算法主要是针对字符串进行加密的方法。具体计算方法如下所示：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
n =&amp; Len(string)\\
hash\_num=&amp;\sum_{i=0}^nseed^{n - i}*string[i]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;id-abtest采用方法&quot;&gt;ABtest采用方法&lt;/h2&gt;
&lt;p&gt;目前ABtest 的imei_level和guid_level使用的hash方式是如下伪代码如下所示：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;hash(string key, string type){
   string union_key = key + type;
   return BKDRhash(union_key, seed = 13131) % 10000;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;id-关于level1与level2这两个hash算法&quot;&gt;关于level1与level2这两个hash算法&lt;/h2&gt;
&lt;p&gt;当使用imei_level1 与 imei_level2 或者 guid_level1与guid_level2时，发现相关性很强：
&lt;img src=&quot;/images/ABtest/1.png&quot; alt=&quot;个数数值&quot; /&gt;(list 很不均匀) list[x*100 + 0-100] 只有4个有数，其余都是0。&lt;/p&gt;

&lt;p&gt;分析：首先imei_level1 与 imei_level2在进行BKDRhash时 union_key的长度都相同即 key+multi_level_one, key+multi_level_two，而且使用的时同一个种子seed = 13131。guid_level同理。那么这样得到的BKDR：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;sum_1 = seed^n*s[0] + seed^{n-1}*s[1] + \cdots+ seed^2*ord(o)+seed^1*ord(n)+ord(e)\\
sum_2 = seed^n*s[0] + seed^{n-1}*s[1] + \cdots+ seed^2*ord(t)+seed^1*ord(w)+ord(o)&lt;/script&gt;

&lt;p&gt;可以看出sum2 - sum1 是一个定值，我们记为$D$,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
设\\
\begin{aligned}
sum_1 =&amp; A\ \  a,b\in[0,99]\ \\
sum_1\pmod {10000} =&amp; A\pmod {10000} = a *100+b\ \\
imei\_level1(key+multi\_level\_one) =&amp; a\\[2ex]
sum_2 =&amp;  A+D \ \ \ q,p\in[0,99]\\
sum_2\pmod {10000} =&amp; (A + D)\pmod {10000} \\
=&amp;(A\pmod {10000} + D\pmod {10000})\pmod {10000}\\ 
=&amp;(a * 100 + b + p*100 + q)\pmod {10000}\\
\end{aligned}\\
\begin{aligned}
\begin{cases}
imei\_level2(key+multi\_level\_two) = (a + p + 1)\ mod \ 100&amp;,\ if\ b + q \ge 100\\
imei\_level2(key+multi\_level\_two) = (a + p)\ mod \ 100&amp;,\ if\ b + q \lt 100 
\end{cases}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;因为无论&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;的值等于多少，&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;的值时固定的。那么&lt;script type=&quot;math/tex&quot;&gt;p,q&lt;/script&gt;的值是固定的。所以会导致如果imei_level1命中了&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;号流量，那么imei_level2要么命中&lt;script type=&quot;math/tex&quot;&gt;(x + p )\ mod \ 100&lt;/script&gt;号，要么命中&lt;script type=&quot;math/tex&quot;&gt;(x + p + 1)\ mod \ 100&lt;/script&gt;号，结论肯定是imei_level1与imei_level2两种hash方式&lt;strong&gt;非常相关&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;但是从结论上看，list[x*100 + 0-100] 命中了4份，而不是刚才推出来的2份。那是因为&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;有两个。ABtest access 最后取值的时候mod了10000的原因。这样子在计算时，会有两个&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;。当进位影响时有一个&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;，没有影响时是另一个&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;。这样产生了两个&lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;，就命中了4份流量。我跑了433260份数据的imei_level1 和imei_level2数据。进位影响的有174900，约占40%，没有影响60%
&lt;img src=&quot;/images/ABtest/2.png&quot; alt=&quot;柱状图&quot; /&gt;
相邻两份命中编号相差1，&lt;script type=&quot;math/tex&quot;&gt;(951+803)/(951+803+135+2498) \approx 40\%&lt;/script&gt;都与推算吻合。&lt;/p&gt;

&lt;h2 id=&quot;id-解决办法&quot;&gt;解决办法&lt;/h2&gt;

&lt;p&gt;只要使D变的无规律即可，&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;如果union_key长度如果不相同，seed的值不相同就可以很好的解决这个问题&lt;/li&gt;
  &lt;li&gt;两种hash方法产生的union_key的长度不相同也可以&lt;/li&gt;
&lt;/ul&gt;

</content>
  </entry>
  
  <entry>
    <title>ThreadPool, Coroutine, Promise与Future</title>
    <link href="http://localhost:4000//2018/08/20/ThreadPool,-Coroutine,-Promise%E4%B8%8EFuture.html"/>
    <updated>2018-08-20T00:00:00+08:00</updated>
    <id>//2018/08/20/ThreadPool, Coroutine, Promise与Future</id>
    <content type="html">&lt;h1 id=&quot;id-threadpool-coroutine-promise与future&quot;&gt;ThreadPool, Coroutine, Promise与Future&lt;/h1&gt;

&lt;h2 id=&quot;id-threadpool&quot;&gt;ThreadPool&lt;/h2&gt;
&lt;p&gt;线程池，顾名思义，就是放着很多线程的大“池子”。主要用作并发量大，但每个任务需要处理的时间不是很长。比如接受或者发送网络请求的任务。之所以使用线程池，原因就是在线程的传创建和销毁在高并发先开销十分大，如果将线程先创建好放入“池子”中待命，随用随取。降低不必要的开销十分划算。&lt;/p&gt;

&lt;p&gt;实现也非常的简单，创建队列，使用生产消费者模型（向队列中增加任务即生产者，将任务执行完成即消费者）。主要实现逻辑如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;ThreadPool::ThreadPool(int thread_size) 
{
    is_stop = false;
    thread_size_ = thread_size;
    pthread_mutex_init(&amp;amp;mutex_, NULL);
    pthread_cond_init(&amp;amp;empty_cond_, NULL);
    for(int i = thread_size_; i &amp;gt; 0; i--) {
        thread *t = new thread([=](){
            while(true){
                pthread_mutex_lock(&amp;amp;mutex_);
                while(task_list_.empty() &amp;amp;&amp;amp; !is_stop) {
                    pthread_cond_wait(&amp;amp;empty_cond_, &amp;amp;mutex_);
                }
                if (is_stop) {
                    pthread_mutex_unlock(&amp;amp;mutex_);
                    return;
                }
                Task task = task_list_.front();
                task_list_.pop();
                pthread_mutex_unlock(&amp;amp;mutex_);
                
                task.f(task.arg);
            }
        });
        thread_vec_.push_back(move(t));
    }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;id-coroutine&quot;&gt;Coroutine&lt;/h2&gt;
&lt;p&gt;协程，又称微线程。这个也名字起的也是很通俗易懂的。就是比线程规模还要小的调度单位。主要用在多并发且I/O开销较高的场景，并且能够减少callback逻辑的使用。在某些不需要并行要求不高的场景下，减少了锁的使用。&lt;/p&gt;

&lt;p&gt;在操作系统中，最小的调度单位就已经是线程了。协程其实就是用户态的线程，即何时调度怎样调度由用户自己控制，而不是由系统控制。操作系统分配给用户一个时间片，用户又将这个时间片再次拆分，更加精细的规划利用。由于用户自己拆分利用，开销就要比系统调度的线程小，操作系统不需要在用户与内核态之间切换，不用更换页表。试想在一个高并发多I/O的任务中，线程遇到的I/O阻塞，被切换调度，另个一线程运行一会也阻塞，这样系统需要维护很多线程，很吃系统的资源。但是在协程中，一旦协程任务遇到了I/O，切换到另一个协程任务，开销只是切换了寄存器和栈，而且还是用户态下的。系统在维护少量线程的情况下能够执行更多的任务，将计算资源高效利用。而且调度是由用户控制，大多数情况下在并发下引起的竞争就不存在了，也就减少了锁的使用。&lt;/p&gt;

&lt;p&gt;协程的实现要比线程池要麻烦一些，主要是因为用户需要在切换时需要自己保存每个任务的状态。在*nix下系统提供了ucontext库，将任务切换时寄存器与栈的切换封装好的。实现的核心如下列代码所示:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
CoroutineBase::CoroutineBase(ucontext_t* main_ctx, void* arg):
		status_(CorStatus::suspend), stack_ptr_(NULL), main_ctx_(main_ctx),
		arg_(arg), yeild_(NULL)
{
	if (main_ctx_ == NULL) return;
	stack_ptr_ = new char[1024 * 32];
	getcontext(&amp;amp;ctx_);
	ctx_.uc_stack.ss_sp = stack_ptr_;
	ctx_.uc_stack.ss_size = 1024 * 32;
	ctx_.uc_stack.ss_flags = 0;
	ctx_.uc_link = main_ctx;
	makecontext(&amp;amp;ctx_, (void (*) (void)) &amp;amp;CoroutineBase::WrapperFunc, 2, this, arg_);
}

void* CoroutineBase::Yeild(void * yeild) {
	if (yeild != NULL) yeild = yeild_;
	status_ = CorStatus::suspend;
	swapcontext(&amp;amp;ctx_, main_ctx_);
	status_ = CorStatus::running;
	return yeild_;
}

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt;
CoroutineMain::CoroutineMain() {
	stack_ptr_ = new char[1024 * 32];
	getcontext(&amp;amp;ctx_main_);
	ctx_main_.uc_stack.ss_sp = stack_ptr_;
	ctx_main_.uc_stack.ss_size = 1024 * 32;
	ctx_main_.uc_stack.ss_flags = 0;
	makecontext(&amp;amp;ctx_main_, (void (*) (void)) &amp;amp;CoroutineMain::Run, 1, this);
}
void CoroutineMain::Run() {
	static int x = 8;
	auto list_it = list_ctx_.begin();
	while(!list_ctx_.empty()) {
		if ((*list_it)-&amp;gt;get_status() == CorStatus::stop) {
			list_it = list_ctx_.erase(list_it);
		} else if ((*list_it)-&amp;gt;get_status() == CorStatus::suspend) {
			ucontext_t* ctx = &amp;amp;(*list_it)-&amp;gt;ctx_;
			(*list_it)-&amp;gt;yeild_ = (void *)&amp;amp;x;
			swapcontext(&amp;amp;ctx_main_, ctx);
			list_it++;
			if (list_it == list_ctx_.end()) list_it = list_ctx_.begin();
		}
	}
}
void CoroutineMain::CreateCoroutine(CoroutineBase *cor) {
	if (cor != NULL) list_ctx_.push_back(cor);
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在实现中我模仿python的yield，在协程中可以传入穿出参数。调试反编译swap可以看到其过程其实就是将现有协程的寄存器的信息保存（保留其上下文），在把要切换的寄存器信息恢复，跟操作系统进程间切换是很相似的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-asm&quot;&gt;(gdb) disassemble
Dump of assembler code for function swapcontext:
=&amp;gt; 0x00007ffff753e400 &amp;lt;+0&amp;gt;:	mov    %rbx,0x80(%rdi)
   0x00007ffff753e407 &amp;lt;+7&amp;gt;:	mov    %rbp,0x78(%rdi)
   0x00007ffff753e40b &amp;lt;+11&amp;gt;:	mov    %r12,0x48(%rdi)
   0x00007ffff753e40f &amp;lt;+15&amp;gt;:	mov    %r13,0x50(%rdi)
   0x00007ffff753e413 &amp;lt;+19&amp;gt;:	mov    %r14,0x58(%rdi)
   0x00007ffff753e417 &amp;lt;+23&amp;gt;:	mov    %r15,0x60(%rdi)
   0x00007ffff753e41b &amp;lt;+27&amp;gt;:	mov    %rdi,0x68(%rdi)
   0x00007ffff753e41f &amp;lt;+31&amp;gt;:	mov    %rsi,0x70(%rdi)
   0x00007ffff753e423 &amp;lt;+35&amp;gt;:	mov    %rdx,0x88(%rdi)
   0x00007ffff753e42a &amp;lt;+42&amp;gt;:	mov    %rcx,0x98(%rdi)
   0x00007ffff753e431 &amp;lt;+49&amp;gt;:	mov    %r8,0x28(%rdi)
   0x00007ffff753e435 &amp;lt;+53&amp;gt;:	mov    %r9,0x30(%rdi)
   0x00007ffff753e439 &amp;lt;+57&amp;gt;:	mov    (%rsp),%rcx
   0x00007ffff753e43d &amp;lt;+61&amp;gt;:	mov    %rcx,0xa8(%rdi)
   0x00007ffff753e444 &amp;lt;+68&amp;gt;:	lea    0x8(%rsp),%rcx
   0x00007ffff753e449 &amp;lt;+73&amp;gt;:	mov    %rcx,0xa0(%rdi)
   0x00007ffff753e450 &amp;lt;+80&amp;gt;:	lea    0x1a8(%rdi),%rcx
   0x00007ffff753e457 &amp;lt;+87&amp;gt;:	mov    %rcx,0xe0(%rdi)
   0x00007ffff753e45e &amp;lt;+94&amp;gt;:	fnstenv (%rcx)
   0x00007ffff753e460 &amp;lt;+96&amp;gt;:	stmxcsr 0x1c0(%rdi)
   0x00007ffff753e467 &amp;lt;+103&amp;gt;:	mov    %rsi,%r12
   0x00007ffff753e46a &amp;lt;+106&amp;gt;:	lea    0x128(%rdi),%rdx
   0x00007ffff753e471 &amp;lt;+113&amp;gt;:	lea    0x128(%rsi),%rsi
   0x00007ffff753e478 &amp;lt;+120&amp;gt;:	mov    $0x2,%edi
   0x00007ffff753e47d &amp;lt;+125&amp;gt;:	mov    $0x8,%r10d
   0x00007ffff753e483 &amp;lt;+131&amp;gt;:	mov    $0xe,%eax
   0x00007ffff753e488 &amp;lt;+136&amp;gt;:	syscall
   0x00007ffff753e48a &amp;lt;+138&amp;gt;:	cmp    $0xfffffffffffff001,%rax
   0x00007ffff753e490 &amp;lt;+144&amp;gt;:	jae    0x7ffff753e4f0 &amp;lt;swapcontext+240&amp;gt;
   0x00007ffff753e492 &amp;lt;+146&amp;gt;:	mov    %r12,%rsi
   0x00007ffff753e495 &amp;lt;+149&amp;gt;:	mov    0xe0(%rsi),%rcx
   0x00007ffff753e49c &amp;lt;+156&amp;gt;:	fldenv (%rcx)
   0x00007ffff753e49e &amp;lt;+158&amp;gt;:	ldmxcsr 0x1c0(%rsi)
   0x00007ffff753e4a5 &amp;lt;+165&amp;gt;:	mov    0xa0(%rsi),%rsp
   0x00007ffff753e4ac &amp;lt;+172&amp;gt;:	mov    0x80(%rsi),%rbx
   0x00007ffff753e4b3 &amp;lt;+179&amp;gt;:	mov    0x78(%rsi),%rbp
   0x00007ffff753e4b7 &amp;lt;+183&amp;gt;:	mov    0x48(%rsi),%r12
   0x00007ffff753e4bb &amp;lt;+187&amp;gt;:	mov    0x50(%rsi),%r13
   0x00007ffff753e4bf &amp;lt;+191&amp;gt;:	mov    0x58(%rsi),%r14
   0x00007ffff753e4c3 &amp;lt;+195&amp;gt;:	mov    0x60(%rsi),%r15
   0x00007ffff753e4c7 &amp;lt;+199&amp;gt;:	mov    0xa8(%rsi),%rcx
   0x00007ffff753e4ce &amp;lt;+206&amp;gt;:	push   %rcx
   0x00007ffff753e4cf &amp;lt;+207&amp;gt;:	mov    0x68(%rsi),%rdi
   0x00007ffff753e4d3 &amp;lt;+211&amp;gt;:	mov    0x88(%rsi),%rdx
   0x00007ffff753e4da &amp;lt;+218&amp;gt;:	mov    0x98(%rsi),%rcx
   0x00007ffff753e4e1 &amp;lt;+225&amp;gt;:	mov    0x28(%rsi),%r8
   0x00007ffff753e4e5 &amp;lt;+229&amp;gt;:	mov    0x30(%rsi),%r9
   0x00007ffff753e4e9 &amp;lt;+233&amp;gt;:	mov    0x70(%rsi),%rsi
   0x00007ffff753e4ed &amp;lt;+237&amp;gt;:	xor    %eax,%eax
   0x00007ffff753e4ef &amp;lt;+239&amp;gt;:	retq
   0x00007ffff753e4f0 &amp;lt;+240&amp;gt;:	mov    0x37a971(%rip),%rcx        # 0x7ffff78b8e68
   0x00007ffff753e4f7 &amp;lt;+247&amp;gt;:	neg    %eax
   0x00007ffff753e4f9 &amp;lt;+249&amp;gt;:	mov    %eax,%fs:(%rcx)
   0x00007ffff753e4fc &amp;lt;+252&amp;gt;:	or     $0xffffffffffffffff,%rax
   0x00007ffff753e500 &amp;lt;+256&amp;gt;:	retq
End of assembler dump.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;id-promise--future&quot;&gt;Promise &amp;amp; Future&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Promise与Future来源于函数式编程，是一种分离计算（Promise）与结果（Future）的范式，在并行化中可以更加灵活的进行计算。在分布式计算中，减少通信往返时引起的延迟，同时这种方式可以使异步程序更直观地表达，而后继传递式（continuation-passing）          –&lt;a href=&quot;https://en.wikipedia.org/wiki/Futures_and_promises&quot;&gt;wiki&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;我从字面上理解就是Promise就是一种承诺，保证去计算，在以后（future）会有答案。主要就是将看似并行的代码用线性的思想写出来。比如计算两个数之和, 用传统的方法去写(伪代码)：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-python&quot;&gt;
atomic int i = 0

def callback():
    i += 1

def GetValue( &amp;amp;rst, callback ):
    rst = net_request_api_latency_2s()
    callback()
    return

def main():
    int a  = 0, b = 0
    thread( GetValue(a, callback) )
    thread( GetValue(b, callback) )
    
    while (i &amp;lt; 2) {}
    
    print(a + b)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果使用Promise与Future 逻辑就不用这么复杂了：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def GetValue():
    rst = net_request_api_latency_2s()
    return rst
    
def main():
    Future a = Promise(GetValue)
    Future b = Promise(GetValue)
    
    print(a.get_value() + b.get_value())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;逻辑非常的清晰。&lt;/p&gt;

&lt;p&gt;从实现角度来看主要就是封装一下thread，寄存一下结果，由future等待获取结果。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;template &amp;lt;class T&amp;gt;
class Promise{
    public:
        explicit Promise(function&amp;lt;T (void*)&amp;gt; func, void *arg) {
            ret_ = new T;
            thread t([=](){*ret_ = func(arg);});
            t_ = move(t);
        }
        Future&amp;lt;T&amp;gt; get_furture() {
            return Future&amp;lt;T&amp;gt;(move(t_), ret_);
        }
        thread t_;
        T *ret_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;template &amp;lt;class T&amp;gt;
class Future {
    public:
    Future(thread&amp;amp;&amp;amp; t, T *ret):t_(move(t)), ret_(ret){};
    Future(Future&amp;lt;T&amp;gt;&amp;amp;&amp;amp;) = default;
    T get() {
        t_.join();
        return *ret_;
    }
    ~Future() {
        delete ret_;
    }
    thread t_;
    T* ret_;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;实验代码在&lt;a href=&quot;https://github.com/zhang-boyang/c-concurrent&quot;&gt;GitHub&lt;/a&gt;上可获取&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>分布式系统哈希分区处理问题</title>
    <link href="http://localhost:4000//2018/07/25/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%93%88%E5%B8%8C%E5%88%86%E5%8C%BA%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98.html"/>
    <updated>2018-07-25T00:00:00+08:00</updated>
    <id>//2018/07/25/分布式系统哈希分区处理问题</id>
    <content type="html">&lt;h1 id=&quot;id-分布式系统哈希分区处理问题&quot;&gt;分布式系统哈希分区处理问题&lt;/h1&gt;

&lt;p&gt;分布式系统的存在，其重要的一个原因就是为了能够将负载很好的均摊到各个节点上，以数量优势提高性能，即分布式系统的可扩展性。大数据可以分布在多个主机的磁盘上，查询也可以有多个主机分别进行处理。分区的实现方式有很多种，目标基本一致，更好的将负载和查询均匀的分布在各个节点。如果分区不均匀(skew)，那么会使此分区甚至整个系统的效率下降。这篇文章主要讨论在高负载下如何避免热点(hot spot)数据。&lt;/p&gt;

&lt;p&gt;本文主要针对我工作中所接触的RTRec流式系统中所使用的分布式系统hash分区做讨论，并从架构的美学(aesthetics)提出一个相对简约分区处理系统。现有流式集群使用一致性哈希，主要有两个原因：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;保证其负载均衡。这里指的是数据层面的负载均衡，因为接入层已经进行了负载均衡。&lt;/li&gt;
  &lt;li&gt;保证处理与查询的key落在同一台机器(节点)上。流式系统一个重要的工作就是收集客户端产生的上报数据做处理。如果要对某一个用户的行为进行上报，最高效的方法就是将这个用户的行为进行收集缓存，统计，入库。这些要求就要保证处理同一个用户的机器是相同的。同理，对APP信息的曝光，下载记录也需要同一key落在同一台机器上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;系统由三部分组成，zookeeper，WatchDog，节点集群。拓扑图如图[1]所示。节点之间的通讯通过一个全局的路由表作为路由，节点集群中的每个节点与zk相连。zk有两个作用，第一个作用是保持路由表的一致性，第二个作用监控集群中的每一个节点的状态。WatchDog的作用是针对现有节点集群做一致性哈希策略，即生成全局的路由表，将路由表放在zk上进行保管，一旦路由表有变化，通知集群中的各个节点去zk上更新最新的路由表。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/consistency_hash/1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[1]:流式系统框架&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;WatchDog生成一致性哈希的策略是将集群组成一个虚拟环状结构(传统的一致性哈希环)，并固定分区数量。如果添加了一个节点，那么就从当前每个节点中抽取一些分区，做到再平衡。同理，如果某些节点被删除或者宕机，那么就将消失的节点以前负责的分区平摊到集群中现存的各个节点上图[2]。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/consistency_hash/2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[2]:当前采用的hash方式&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;这种哈希方式的优点是每次节点变动后的分区个数总能做到最优的平衡状态，分区的大小十分的均匀。但是这也带来了一个问题，必须需要一个WatchDog一样的角色为这个集群生成这张统一的哈希路由表。如果由每个节点自己生成，会由于节点变更的时序问题导致生成的哈希表不一致。图[3],这个例子说明了，相同的两个节点但顺序不同的变更状态导致了最终生成的路由表不同的例子。而且在分布式集群中，每个集群都需要一个WatchDog进行分配，WatchDog这一模式又回到了传统的server-client的集中式设计上，如果WatchDog宕机，就会导致整个集群无法更新路由表，严重时导致集群不能正常工作，可用性大大降低。如果给WatchDog一个备份，又回到了分布式系统一致性的问题上。总之WatchDog角色大大降低了整个分布式系统的可用性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/consistency_hash/3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[3]:当节点收到消息先后顺序不同时对哈希表的影响&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;在这里受到对象存储的启发，使用另一种一致性哈希方法。以去掉WatchDog，更好的保护此分布式系统可用性的角度出发，更充分的去中心化，充分利用zk的功能，采用哈希计算的策略保证系统的可用性与整体设计的简约。&lt;/p&gt;

&lt;p&gt;具体方法：同样固定分区数量(BASE)，每个节点在初始化时在zk创建临时(ephemeral)、自增计数(Sequence)节点，并订阅此目录。由zk生成的计数作为，每一个节点就获得到了一个全局唯一的ID。订阅的目的是，如果在某一订阅目录下有节点发生变动，会通知所有订阅的节点。每个节点内部采用虚拟节点(virtual_index)使得分布变得更加均匀。通过&lt;code&gt;HASH(ID, virtual_index) mod BASE&lt;/code&gt;这一公式计算出每个节点所负责的分区。如图[4]的例子，每个节点较为均匀进行了分区。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/consistency_hash/4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[4]:使用计算生成的hash表在一致性哈希环上的分布&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;只要每一个节点使用的为同一个公式，那么所有节点自身生成的路由表都是相同的，不需要WatchDog这一角色进行统一生成分区表，因为&lt;strong&gt;路由表生成公式的计算与节点先后加入的顺序无关&lt;/strong&gt;。其实与分布式文件系统类似，使用WatchDog就是文件存储，使用Hash函数映射分区就是对象存储。而WatchDog在文件系统中的角色就是元数据服务器，Hash函数映射作用就是类似ceph文件系统中的CRUSH。&lt;/p&gt;

&lt;p&gt;在使用哈希计算这一方法进行分区时，需要考虑的一点就是哈希函数的计算公式与虚拟节点设置的个数。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;哈希函数计算必须能够尽可能做到打散，一般使用MD5或者是SHA等这种优秀常见的方式。&lt;/li&gt;
  &lt;li&gt;虚拟节点的个数设置不易过少，如果过少，极有可能导致分局数量不均匀，从而不能做到很好的负载均衡。而虚拟节点也不是灵丹妙药，在固定的分区数量中，如果虚拟节点数越多那么在生成虚拟节点时，发生碰撞的概率也就越大，密码学中称为生日碰撞攻击,公式：&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varepsilon \approx 1 - \frac{(\frac{N}{N-x})^{N-x+\frac{1}{2}}}{e^x}&lt;/script&gt;

&lt;p&gt;ε为碰撞概率，N为基数(base)大小，x为总虚拟节点个数&lt;/p&gt;

&lt;p&gt;(这个公式是我用斯特林公式推倒出来，懒得化简。可能跟网上查出的生日碰撞格式有所出入，不过计算值是相同的),同时大量的虚拟节点增加了无用的计算量，浪费了生成路由表与节点间路由转发的时间与资源。图[5]为当分区个数设为4096，节点数为10时虚拟节点对系统的影响。橙色线条是节点虚拟个数与每个实体节点中所包含的分区数量方差的曲线图。蓝色为碰撞概率与虚拟节点数之间的关系曲线。可以看出，当每个分区的虚拟分区大于10个后，方差下降的幅度就很小了，在节点接近15个左右的时候，碰撞概率就达到90%了(密度已经很大了)，再往上加虚拟节点造成的各种开销增大，而起到的作用很小，性价比极低。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/consistency_hash/5.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[5]:虚拟个数与每个实体节点中所包含的分区数量方差的曲线图和碰撞概率与虚拟节点数之间的关系曲线图&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;从图中可以明显地得出在每个节点10个虚拟节点时起到的效果最好。&lt;/p&gt;

&lt;p&gt;实验代码在&lt;a href=&quot;https://github.com/zhang-boyang/consistency_hash&quot;&gt;github&lt;/a&gt;中获得(做实验时需安装zookeeper服务)&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>6.824:Lab6-7 Replicated State Machine</title>
    <link href="http://localhost:4000//2018/05/22/6.824-Lab6-7-Replicated-State-Machine.html"/>
    <updated>2018-05-22T00:00:00+08:00</updated>
    <id>//2018/05/22/6.824:Lab6-7-Replicated-State-Machine</id>
    <content type="html">&lt;h1 id=&quot;id-paxos&quot;&gt;Paxos&lt;/h1&gt;
&lt;h2 id=&quot;id-introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;在实验6-7中，将使用RSM（replicated state machine）的方法复写锁服务，在这个方法中，有一个节点为master，master接收来自client节点的请求并在所有的节点以相同的方式在所有的副本下执行。&lt;/p&gt;

&lt;p&gt;当master节点失败，任何一个副本都会接管master的工作，因为这些副本都与那个失败的节点有着相同的状态。其中一个比较有挑战性任务时确保集群中的每一个节点所接受的信息是一致的（即哪些是副本，谁是master，哪些副本是keepalive的），即使存在网络分裂（network partition）或者是乱序的情况，数据仍然要保持一致。这里我们使用Paxos协议来实现这一策略。&lt;/p&gt;

&lt;p&gt;在此次实验中，将要实现Paxos并使用达成集群成员的改变（view change）。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RSM module&lt;/p&gt;

    &lt;p&gt;RSM负责副本管理，当一个节点加入时，RSM模块直接配置并添加。RSM模块也是管理recovery线程确保在相同的view下每一个节点的状态是相同。这个本实验中唯一需要recovery Paxos操作。在实验7中，RSM模块将管理锁服务。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;config module&lt;/p&gt;

    &lt;p&gt;config模块负责管理view的改变。当RSM模块要求在当前view上添加一个节点时，config模块将使用Paxos来开启一个新view。config模块将周期性的发送心跳来检查集群中的其他节点是否存活，如果某一成员不需要失联，就讲此节点从当前的view中移除出去，移除节点也会使用paxos协议来达成一个新的view，这个view的value中移除了那个失联节点。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paxos模块&lt;/p&gt;

    &lt;p&gt;Paxos模块负责运行Paxos协议来维持值一直。原则上这个值（value）可以是任何集群想要达成一直的value（当前成员节点，每一次操作的日志 etc.）。在我们的系统中，将value作为当前集群的节点成员。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;id-uderstanding-how-paxos-is-used-for-view-changes&quot;&gt;Uderstanding how Paxos is used for view changes&lt;/h2&gt;
&lt;p&gt;Paxos的实现使用了两个类，acceptor与proposer。每一个副本都有这两个类。proposer类通过提出新value的方式来发起一个paxos协议，并发送给所有的副本。acceptor类处理来自proposer的请求，并作出反馈。proposer中的run方法获取现有的nodes值，并达成一个新的value值。当一个新的instance（epoch）完成时，acceptor将调用config的paxos_commit(instance, v)方法进行提交。具体流程如下列所示：
系统从头开始，第一个节点创建了view 1，仅仅包含了它自己，view_1 = {1}.({values})。当节点2加入时，两节点的RSM模块加入了1，并从节点1的中得到了view。然后，节点2要求它的config模块奖自己加入view1，config模块调用Paxos发起一个新的view:view_2 包含了节点1与2。当paxos成功之后 view_2为 view_2={1,2}。同理当节点3进入时，通过Paxos后view_3 = {1,2,3}。&lt;/p&gt;

&lt;h2 id=&quot;id-paxos-protocal&quot;&gt;Paxos Protocal&lt;/h2&gt;
&lt;p&gt;实验指导书上已经将propose和acceptor的具体框架写好。为代码如下：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    proposer run(instance, v):
     choose n, unique and higher than any n seen so far
     send prepare(instance, n) to all servers including self
     if oldinstance(instance, instance_value) from any node:
       commit to the instance_value locally
     else if prepare_ok(n_a, v_a) from majority:
       v' = v_a with highest n_a; choose own v otherwise
       send accept(instance, n, v') to all
       if accept_ok(n) from majority:
         send decided(instance, v') to all
    
    acceptor state:
     must persist across reboots
     n_h (highest prepare seen)
     instance_h, (highest instance accepted)
     n_a, v_a (highest accept seen)
    
    acceptor prepare(instance, n) handler:
     if instance &amp;lt;= instance_h
       reply oldinstance(instance, instance_value)
     else if n &amp;gt; n_h
       n_h = n
       reply prepare_ok(n_a, v_a)
     else
       reply prepare_reject
    
    acceptor accept(instance, n, v) handler:
     if n &amp;gt;= n_h
       n_a = n
       v_a = v
       reply accept_ok(n)
     else
       reply accept_reject
    
    acceptor decide(instance, v) handler:
     paxos_commit(instance, v)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;其实这一实验的中心就是实验Paxos协议。propser和acceptor框架已经搭好，所需要做的就是将其成员函数补齐。伪代码已经给出，剩下的就是实现与调试。&lt;/p&gt;

&lt;h1 id=&quot;id-replicated-state-machine&quot;&gt;Replicated State machine&lt;/h1&gt;
&lt;h2 id=&quot;id-introduction-1&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;在这个实验中奖使用RSM（复制状态机）的方法实现锁服务的副本管理。在RSM方法里使用一主多备份（one master， others slaves）。master节点负责处理来自客户端（client）的请求并在都有副本上执行这些请求。为了保证所有副本完全一致，这些副本的执行顺序必须完全有序，所有请求的所有结果必须完全一致。RSM使用Paxos协议处理节点成员之间的变更（如失败和重新加入副本等）&lt;/p&gt;

&lt;p&gt;为了确保所有的请求的顺序是唯一的，主节点给每一个请求分配一个viewstamp以保证序列。viewstamp又两个部分组成，一个为view number（paxos协议）和一个单调递增的序列号（seq no）。一般而言，viewstamp中的view number由高到低排序，seq no也是有高到低排序。怎样保证viewstamps是唯一的呢？这是因为Paxos保证左右的viewnumber是一个有序的。另外对于每一个view，现有的view成员是一致的，所以RSM节点可以保证一个唯一的master，只有它可以给每一个请求一个递增的seqno来保证在一个view中请求的顺序性。&lt;/p&gt;

&lt;p&gt;这一实验的首要任务是在我们现有的RPC库之上构建一个RSM库，以确保副本的一致性。有一些约束的调节来确保所有副本以相同的顺序执行相同的请求得到相同的结果。一旦你已经构建好了RSM库，我们将要求你使用RSM对锁服务进行副本化。&lt;/p&gt;

&lt;h2 id=&quot;id-start&quot;&gt;Start&lt;/h2&gt;
&lt;p&gt;首先提供了rsm_client 与 rsm 这两个文件。RSM_clinet主要确定并与服务的master节点进行通信，lock_client可以使用call的方法调用RSM的这一方法（invoke）。&lt;/p&gt;

&lt;p&gt;为了将所有的服务的副本化，你服务必须创建一个RSM服务对象，并使用他来处理来自客户端的RPC。并使用config对服务进行Paxos的一致性保障。&lt;/p&gt;

&lt;h2 id=&quot;id-your-job&quot;&gt;Your Job&lt;/h2&gt;
&lt;p&gt;你的工作是将锁缓存服务构建在RSM服务之上，通过rsm_test.pl 8-16进行测试。&lt;/p&gt;

&lt;h3 id=&quot;id-step-one-副本化锁缓存服务&quot;&gt;Step One 副本化锁缓存服务&lt;/h3&gt;
&lt;p&gt;重构lab4中的锁缓存服务包括客户端与服务端（lock_server_cache_rsm 与 locker_client_cache_rsm）&lt;/p&gt;

&lt;p&gt;在实现锁缓存服务副本化时应当考虑三个问题。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;server端（SVR_HDL）不要直接去调用client端（SLT_HDL），会发生死锁（RSM层的invoke_mutex 死锁）。C_A向S请求acquire L1（申请了invoke_mutex），而这时L1在C_B上，S又向C_B发起revoke，但是上一步（acquire L1还未完成）S拿着invoke_mutex，这样会产生死锁。所以不可在调用中再调用。为了解决这个问题，使用多个线程进行处理。client使用 releaser 这个线程专门进行release处理，server使用revoker和retryer 对客户端进行revoke和retry调用。&lt;/p&gt;

    &lt;p&gt;以客户端为例，基本逻辑为利用rpc/fifo.h提供的队列，连接RPC handler和background（releaser etc.）线程之间的通信。fifo队列中一个pthread_cond_t，当队列为空的时候休眠，当队列有东西的时候被唤醒，有点像生产者和消费者模式。handler作为生产者将需要处理的item放入到队列中，releaser作为消费者，一旦在队列中存在item，就对此item进行处理。见下列代码：&lt;/p&gt;

    &lt;pre&gt;&lt;code class=&quot;language-c++&quot;&gt; //This is a producer
 void lock_client_cache_rsm::DealRelease(ClientLockRSM &amp;amp; lock_item, lock_protocol::lockid_t lid)
 {
     if (lock_item.status == AcqRet::NONE) {
         //primary done, req will resend, so this situation will happend
         //release_queue_.enq(&amp;amp;lock_item);
         return;
     }
     if (lock_item.is_revoked){
         lock_item.status = AcqRet::RELEASING;
         release_queue_.enq(&amp;amp;lock_item);
     }else{
         lock_item.status = AcqRet::FREE;
         lock_item.is_finished = true;
     }
     pthread_cond_broadcast(&amp;amp;lock_item.wait_free_);
     pthread_cond_broadcast(&amp;amp;lock_item.wait_acq_);
 }
&lt;/code&gt;&lt;/pre&gt;
    &lt;pre&gt;&lt;code&gt; //This is a consumer
 void
 lock_client_cache_rsm::releaser()
 {
    
   // This method should be a continuous loop, waiting to be notified of
   // freed locks that have been revoked by the server, so that it can
   // send a release RPC.
   do{
       ClientLockRSM * lock_item_ptr = NULL;
       release_queue_.deq(&amp;amp;lock_item_ptr);
       VERIFY(lock_item_ptr != NULL);
       pthread_mutex_lock(&amp;amp;lock_map_mutex_);
       lock_protocol::lockid_t lid = lock_item_ptr-&amp;gt;lid;
       lock_protocol::xid_t xid_ = lock_item_ptr-&amp;gt;xid;
       if (lu != NULL) lu-&amp;gt;dorelease(lid);
       int r;
       pthread_mutex_unlock(&amp;amp;lock_map_mutex_);
       lock_protocol::status ret = rsmc-&amp;gt;call(lock_protocol::release, lid, id, xid_, r);
       tprintf(&quot;I send it %llu back to server\n&quot;, lid);
       VERIFY(ret == lock_protocol::OK);
       pthread_mutex_lock(&amp;amp;lock_map_mutex_);
       lock_item_ptr-&amp;gt;is_finished = false;
       lock_item_ptr-&amp;gt;is_revoked = false;
       lock_item_ptr-&amp;gt;status = AcqRet::NONE;
       pthread_cond_signal(&amp;amp;lock_item_ptr-&amp;gt;wait_free_);
       pthread_mutex_unlock(&amp;amp;lock_map_mutex_);
   }while(true);
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;同理在锁服务端还有一个revoker和retryer，与其类似。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;缓冲锁服务的客户端应当有能力处理锁服务的失败，因为一旦primary（master）节点失败选出新的primary节点后，客户端是不清楚老的primary节点是否已经处理了自己的请求。&lt;/p&gt;

    &lt;p&gt;为了处理这一问题，客户端要给所有自己的请求一个系列号，此序列号非比序列号（非rsm viewstamp中的seqno），每一个请求有一个唯一的用户ID（这里其实用的就是与Lab4一样的IP端口号二元组），对于一个acquire请求，客户端选出一个未使用的seqno，并将其seqno作为参数参入到请求中。release同样需要将这个seqno传入。使用这一方法就需要客户端记住每一个锁每一个用户的最大seqno，这里我将锁结构加入一个map记录这一信息(lock_info_)&lt;/p&gt;
    &lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt; struct CondLockCacheRSM : public CondLock{
     //Recode every user_id with it's highest
     std::map&amp;lt;std::string, lock_protocol::xid_t&amp;gt; lock_info_;
     std::queue&amp;lt;std::string&amp;gt; wait_retry_queue_;
     std::queue&amp;lt;std::string&amp;gt; wait_revoke_queue_;
     lock_protocol::lockid_t lid;
     std::string user_id;
 };
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;&lt;strong&gt;服务端应该丢弃掉过时的请求，但是必须回复一个来自相同客户端重复信息的请求。&lt;/strong&gt;
 这里我们使用lock_protocol::lockid_t xid_t 作为seqno。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果没在没有失败的情况下，所有服务起的所状态应当是一致的。如果要达到这一条件，必须要满足一下两点：&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;当input作用于所有的副本之上，且作用方式要相同，（才能得到相同的状态）&lt;/li&gt;
      &lt;li&gt;当input只有primary承担时，不应该改变primary的状态&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;换句话说就是primary与non-primary（master与slaves）之间的状态在没有失败的情况下应当是完全相同的。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在完成了次步骤后使用Lab1当时测试lock的locker_test应当要passed all tests sucessfully才可以&lt;/p&gt;

&lt;h2 id=&quot;id-setp-tworsm-without-failures&quot;&gt;Setp Two:RSM Without Failures&lt;/h2&gt;
&lt;p&gt;第二步的任务是在第一步锁服务正常的情况下，使用RSM机制。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RSM客户端通过调用invoke函数请求master节点&lt;/li&gt;
  &lt;li&gt;master节点分配client请求一个viewstamp，然后将此请求再发送给每一个slave&lt;/li&gt;
  &lt;li&gt;如果次请求的viewstamp正常，slave将执行这个请求并返回OK给master&lt;/li&gt;
  &lt;li&gt;master收到所有的slave都返回正常（&lt;code&gt;rsm_client_protocol::OK&lt;/code&gt;）后，在本地执行并将回复发送给客户端，如图[1]:&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/distribution_pic/acq_ok.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[1] 副本化客户端请求服务端逻辑&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;具体的一些细节如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;rsm::client_invoke中如果paxos的view正在改变，应当返回rsm_client_protocol::BUSY告诉客户端稍后重试，如果RSM已经不再是此集群的master节点，应当返回rsm_client_protocol::NOTPRIMARY，client会调用init_members()，重新选出一个master节点。如果一切正常，分配一个seqno，将自己的viewstamp和req发送给其他slave，（&lt;code&gt;cl-&amp;gt;call(rsm_protocol::invoke, procno, last_myvs, req, dummy, rpcc::to(1000));&lt;/code&gt;）。确认所有slave都ok后，自己调用execute()。&lt;/p&gt;

    &lt;p&gt;如果有一个slave返回不OK的时候（或者失败）的时候，master节点不执行此次请求，返回rsm_client_protocol::BUSY，此时失败的slave节点的seqno将与其他正常节点的不相同，导致其进行recovery（heardbeat失去联系，remove_wo）。如图[2]所示:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/images/distribution_pic/acq_fail.png&quot; alt=&quot;image&quot; /&gt;
  &lt;em&gt;图[2] 有副本节点失败的&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;具体代码如下：&lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;  rsm_client_protocol::status
  rsm::client_invoke(int procno, std::string req, std::string &amp;amp;r)
  {
    int ret = rsm_client_protocol::OK;
    // You fill this in for Lab 7
    ScopedLock inv_m(&amp;amp;invoke_mutex);
    ScopedLock rsm_m(&amp;amp;rsm_mutex);
    
    if (inviewchange){
        ret = rsm_client_protocol::BUSY;
    }else if (primary != cfg-&amp;gt;myaddr()) {
        ret = rsm_client_protocol::NOTPRIMARY;
    }
    if (ret != rsm_client_protocol::OK) return ret;
    
    bool is_fail = false;
    last_myvs = myvs;
    myvs.seqno++;
    int index = 0;
    std::vector&amp;lt;std::string&amp;gt; view = cfg-&amp;gt;get_view(vid_commit);
    for(typeof(view.begin()) it = view.begin(); it != view.end(); it++){
        if (*it == primary) continue;
        int dummy;
        rsm_protocol::status rpc_r = rsm_protocol::ERR;
        handle h(*it);
        rpcc *cl = h.safebind();
        pthread_mutex_unlock(&amp;amp;rsm_mutex);
        if (cl){
            rpc_r = cl-&amp;gt;call(rsm_protocol::invoke, procno, last_myvs, req, dummy, rpcc::to(1000));
        }
        pthread_mutex_lock(&amp;amp;rsm_mutex);
        if (rpc_r != rsm_protocol::OK){
            is_fail = true;
            tprintf(&quot;I am here is_false=true\n&quot;);
            break;
        }
        if (index++ == 0){
            breakpoint1();
            partition1();
        }
    }
    
    if (is_fail) return rsm_client_protocol::BUSY;
    
    execute(procno, req, r);
    
    return ret;
  }
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;lock_tester中使用cache_rsm调用。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;只有primary节点才与client节点通信，其余的副本slave节点对于retry和rovoke的行为保持沉默就好，&lt;code&gt;if (rsm-&amp;gt;amiprimary())&lt;/code&gt;在retryer和revoker中加入判断，如果非primary就当作什么也没有发生，primary&lt;code&gt;call(rlock_protocol::retry, lid, xid, r);&lt;/code&gt;与客户端进行通行。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;客户端与服务端进行通行的时候，不能直接使用lock_server在RPC上注册的函数，而要使用rsm_client的invoke于服务段rsm_server中的client_invoke通行，好在实验代码以提供了这样的函数并完成，只是在调用时注意。以请求客户端的acquire为例：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  int lock_client_cache_rsm::SendAcqToSvr(ClientLockRSM &amp;amp; lock_item, lock_protocol::lockid_t lid){
      int r;
      xid++;
      lock_item.is_retried  = false;
      lock_item.xid = xid;
      pthread_mutex_unlock(&amp;amp;lock_map_mutex_);
      int ret = rsmc-&amp;gt;call(lock_protocol::acquire, lid, id, xid ,r);
      pthread_mutex_lock(&amp;amp;lock_map_mutex_);
      tprintf(&quot;I sent a msg to svr, ret=%d, xid=%lld, ret=%d\n&quot;, r, xid, ret);
      if(ret != lock_protocol::OK) return ret;
      if (r == AcqRet::OK){
          lock_item.status = AcqRet::FREE;
          tprintf(&quot;%s said OK lid:%lld\n&quot;, rsmc-&amp;gt;primary.c_str(), lid);
      } else if (r == AcqRet::RETRY) {
          lock_item.status = AcqRet::ACQING;
      } else {
          tprintf(&quot;unknow svr return %d\n&quot;, ret);
      }
      return ret;
    
  }
&lt;/code&gt;&lt;/pre&gt;
    &lt;h2 id=&quot;id-step-threecope-with-backup-failures-and-implement-state-transfer&quot;&gt;Step Three:Cope With Backup Failures and Implement State Transfer&lt;/h2&gt;
    &lt;p&gt;这一步需要处理备份失败后。当发一线一个失败的节点或者是有一个新的节点加入时，这里就需要考虑Paxos协议。这些行为会调用commit_change()达成一个新的view，当新的view形成时，inviewchange为true，表明了这个节点在正常工作前需要通过RSM状态恢复（recovery）。recovery函数已实现。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所有的副本应当从master节点通过转移状态（transferstate）恢复。这个状态应当与primary的一模一样。当recovery结束时，将inviewchange值置为false，从而允许处理请求，在所有的副本恢复完成前，primary节点不应当接受与发送任何的请求给副本。&lt;/p&gt;

&lt;p&gt;实现状态转移，最主要的是要实现marshal_state和unmarsh_state方法。（其实就是将primary那个装lock信息的map序列化后传给slave，然后slave再反序列化成自己的lock信息）这里可以使用rpc通信中的marshall和unmarshall类，« 已经重载很方便。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;std::string
lock_server_cache_rsm::marshal_state()
{
  marshall rep;
  ScopeLock ml(&amp;amp;m_lock_mutex_);

  rep &amp;lt;&amp;lt; (unsigned int)map_lock_.size();
  foreach(map_lock_, lock_it){
      rep &amp;lt;&amp;lt; lock_it-&amp;gt;first;
      rep &amp;lt;&amp;lt; lock_it-&amp;gt;second.is_locked;
      rep &amp;lt;&amp;lt; lock_it-&amp;gt;second.lid;
      //rep &amp;lt;&amp;lt; map_it-&amp;gt;second.pcond; pcond is not used after lab4
      rep &amp;lt;&amp;lt; lock_it-&amp;gt;second.user_id;

      rep &amp;lt;&amp;lt; (unsigned int)lock_it-&amp;gt;second.lock_info_.size();
      foreach(lock_it-&amp;gt;second.lock_info_, info_it){
          rep &amp;lt;&amp;lt; info_it-&amp;gt;first;
          rep &amp;lt;&amp;lt; info_it-&amp;gt;second;
      }

      std::queue&amp;lt;std::string&amp;gt; tmp_queue = lock_it-&amp;gt;second.wait_retry_queue_;
      rep &amp;lt;&amp;lt; (unsigned int)tmp_queue.size();
      while(!tmp_queue.empty()){
          rep &amp;lt;&amp;lt; tmp_queue.front();
          tmp_queue.pop();
      }

      tmp_queue = lock_it-&amp;gt;second.wait_revoke_queue_;
      rep &amp;lt;&amp;lt; (unsigned int)tmp_queue.size();
        while(!tmp_queue.empty()){
        rep &amp;lt;&amp;lt; tmp_queue.front();
        tmp_queue.pop();
      }
  }

  return rep.str();
}

void
lock_server_cache_rsm::unmarshal_state(std::string state)
{
    unmarshall rep(state);
    ScopeLock ml(&amp;amp;m_lock_mutex_);
    map_lock_.clear();
    unsigned int lock_size = 0;
    rep &amp;gt;&amp;gt; lock_size;
    for(unsigned int i = 0; i &amp;lt; lock_size; i++){
        lock_protocol::lockid_t lockid = 0;
        rep &amp;gt;&amp;gt; lockid;
        CondLockCacheRSM&amp;amp; lock_item = map_lock_[lockid];
        rep &amp;gt;&amp;gt; lock_item.is_locked;
        rep &amp;gt;&amp;gt; lock_item.lid;
        rep &amp;gt;&amp;gt; lock_item.user_id;

        unsigned int info_size = 0;
        rep &amp;gt;&amp;gt; info_size;
        std::map&amp;lt;std::string, lock_protocol::xid_t&amp;gt;&amp;amp; info_item = lock_item.lock_info_;
        info_item.clear();
        for(unsigned int j = 0; j &amp;lt; info_size; j++){
            std::string key;
            lock_protocol::xid_t value;
            rep &amp;gt;&amp;gt; key;
            rep &amp;gt;&amp;gt; value;
            info_item[key] = value;
        }

        unsigned int retry_size = 0;
        rep &amp;gt;&amp;gt; retry_size;
        std::queue&amp;lt;std::string&amp;gt;&amp;amp; retry_queue = lock_item.wait_retry_queue_;
        {
            //clear the queue
            std::queue&amp;lt;std::string&amp;gt; empty_queue;
            std::swap(retry_queue, empty_queue);
        }
        for(unsigned int k = 0; k &amp;lt; retry_size; k++){
            std::string item;
            rep &amp;gt;&amp;gt; item;
            retry_queue.push(item);
        }

        unsigned int revoke_size = 0;
        rep &amp;gt;&amp;gt; revoke_size;
        std::queue&amp;lt;std::string&amp;gt;&amp;amp; revoke_queue = lock_item.wait_revoke_queue_;
        {
            //clear the queue
            std::queue&amp;lt;std::string&amp;gt; empty_queue;
            std::swap(revoke_queue, empty_queue);
        }
        for(unsigned int l = 0; l &amp;lt; revoke_size; l++){
            std::string item;
            rep &amp;gt;&amp;gt; item;
            revoke_queue.push(item);
        }
    }

}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后需要同步状态时处理以下几个函数：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;rsm::sync_with_backups    //就是为了找出现在所有的backup，并等待他们全部recovery结束&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  bool
  rsm::sync_with_backups()
  {
    pthread_mutex_unlock(&amp;amp;rsm_mutex);
    {
      // Make sure that the state of lock_server_cache_rsm is stable during 
      // synchronization; otherwise, the primary's state may be more recent
      // than replicas after the synchronization.
      ScopedLock ml(&amp;amp;invoke_mutex);
      // By acquiring and releasing the invoke_mutex once, we make sure that
      // the state of lock_server_cache_rsm will not be changed until all
      // replicas are synchronized. The reason is that client_invoke arrives
      // after this point of time will see inviewchange == true, and returns
      // BUSY.
    }
    pthread_mutex_lock(&amp;amp;rsm_mutex);
    // Start accepting synchronization request (statetransferreq) now!
    insync = true;
    // You fill this in for Lab 7
    // Wait until
    //   - all backups in view vid_insync are synchronized
    //   - or there is a committed viewchange
    backups = cfg-&amp;gt;get_view(vid_insync);
      for(typeof(backups.begin()) it = backups.begin(); it != backups.end(); ){
          if (*it == primary)
             it = backups.erase(it);
          else
             it++;
      }
      while(vid_insync == vid_commit &amp;amp;&amp;amp; !backups.empty())
          pthread_cond_wait(&amp;amp;recovery_cond, &amp;amp;rsm_mutex);
      insync = false;
      return true;
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;rsm::sync_with_primary    //  slave节点，调用statetransfer进行状态转移（从primary那里marsh lock map，再unmarshall到自己的lock map 中）
    &lt;pre&gt;&lt;code&gt;  bool
  rsm::sync_with_primary()
  {
    // Remember the primary of vid_insync
    std::string m = primary;
    // You fill this in for Lab 7
    // Keep synchronizing with primary until the synchronization succeeds,
    // or there is a commited viewchange
    
    bool ret = false;
      while(!ret &amp;amp;&amp;amp; vid_insync == vid_commit)
          ret = statetransfer(m);
    
      ret = statetransferdone(m);
    
      return ret;
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;rsm::statetransferdone    调用transferreq
    &lt;pre&gt;&lt;code&gt;  bool
  rsm::statetransferdone(std::string m) {
    // You fill this in for Lab 7
    // - Inform primary that this slave has synchronized for vid_insync
    pthread_mutex_unlock(&amp;amp;rsm_mutex);
    int r = 0;
    int ret = 0;
    handle h(m);
    rpcc *cl = h.safebind();
    
    if (cl) {
        ret = cl-&amp;gt;call(rsm_protocol::transferdonereq, cfg-&amp;gt;myaddr(), vid_insync, r);
    }
    pthread_mutex_lock(&amp;amp;rsm_mutex);
    if (cl &amp;amp;&amp;amp; ret == rsm_protocol::OK)
        return true;
    else
        return false;
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;rsm::transferdonereq  // 如果所有的backup全部recovery完毕，唤醒sync_with_backups()函数中等待的primary
    &lt;pre&gt;&lt;code&gt;  rsm_protocol::status
  rsm::transferdonereq(std::string m, unsigned vid, int &amp;amp;)
  {
    int ret = rsm_protocol::OK;
    ScopedLock ml(&amp;amp;rsm_mutex);
    // You fill this in for Lab 7
    // - Return BUSY if I am not insync, or if the slave is not synchronizing
    //   for the same view with me
    // - Remove the slave from the list of unsynchronized backups
    // - Wake up recovery thread if all backups are synchronized
    
    if (insync == false || vid != vid_insync)
        return rsm_protocol::BUSY;
    
    for(typeof(backups.begin()) it = backups.begin(); it != backups.end(); ){
        if (*it == m)
            it = backups.erase(it);
        else
            it++;
    }
    if (backups.empty())
        pthread_cond_broadcast(&amp;amp;recovery_cond);
    
    return ret;
  }
&lt;/code&gt;&lt;/pre&gt;
    &lt;h2 id=&quot;id-step-fourcope-with-primary-failures&quot;&gt;Step Four:Cope with Primary Failures&lt;/h2&gt;
    &lt;p&gt;之是在在primary不变的情况下，处理backup（slaves, replicas…），现在遇到的情况是当primary失败后的情景。invoke函数了两个特殊的情况：&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;一个是当primary不再是primary时，返回NOPRIMARY的情况，这种情况下，client会调用init_members函数&lt;code&gt;cl-&amp;gt;call(rsm_client_protocol::members, 0, new_view, rpcc::to(1000));&lt;/code&gt;，这个函数会返回一组新的集群信息，将返回vector的倒数第一个作为当前primary，再去请求。&lt;/li&gt;
  &lt;li&gt;第二个是当前的primary节点无返回信息，即当前primary失败。这是需要客户端在已知的view vector中取倒数第一个作为当前primary，再去请求。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;step4 出现的这种primary失败的情况就体现出了我们lock server记录每一个client请求时最大值（xid）的意义了。当primary失效后，client和backup（slave）都不知道上一个请求到底有没有被处理（slave 无法知道 primary是在那个点宕机的）。如果处理了，而client以为没有处理，那么这就会导致duplicated request。加上对比某锁某用户的xid，如果比这次请求的xid小那么就是重复请求，新primary就会发现到。图[3]:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/distribution_pic/primary_fail.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;在图3中，在retry或者revoke时primary节点宕机，client_x处在等待的状况下，都会发生无法唤醒client_x的情况&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;还有一个点应当注意的是以下情况：当primary给一个client发送retry时，可能会在此时failure，新的primary也不会再给client发retry，这样这个client会在等待retry时停滞。这里解决的方法比较简单粗暴，就是每隔三秒就发一次，不管retry来没有来。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    gettimeofday(&amp;amp;tp, NULL);
    ts.tv_sec = tp.tv_sec + WAIT_TIMES;
    ts.tv_nsec = tp.tv_usec * 1000;
    ret = pthread_cond_timedwait(&amp;amp;lock_item.wait_acq_, &amp;amp;lock_map_mutex_, &amp;amp;ts);
    if (ret == ETIMEDOUT) lock_item.is_retried = true;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样，另一方的client也许会收到很多revoke的信息，即使当它已经不再拥有此锁。不过还好，有xid，我们可以判断这个lock到底应当怎样处理。&lt;/p&gt;

&lt;h2 id=&quot;id-step-five-complicated-failures&quot;&gt;Step Five: Complicated Failures&lt;/h2&gt;
&lt;p&gt;此step没有需要做的一些实质性操作，在指定的位置加入breakponit 和 parition1 进行测试。需要测试./rsm_tester.pl 12 13 14 15 16&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>6.824:Lab4-5 Cache</title>
    <link href="http://localhost:4000//2018/05/22/6.824-Lab4-5-Cache.html"/>
    <updated>2018-05-22T00:00:00+08:00</updated>
    <id>//2018/05/22/6.824:Lab4-5-Cache</id>
    <content type="html">&lt;h1 id=&quot;id-lab4-5-caching-locks-and-extents&quot;&gt;Lab4-5 Caching Locks and Extents&lt;/h1&gt;
&lt;h2 id=&quot;id-introductionlab4&quot;&gt;Introduction(Lab4)&lt;/h2&gt;
&lt;p&gt;在这两个实验中，主要建立了锁和存储服务的缓存，以减少服务器负载并提高客户端的性能。&lt;/p&gt;

&lt;p&gt;例如在Lab3中的测试，在一个YFS文件夹中建立100个文件，需要像这个文件夹（directory）的锁请求100次。这次的实验就是修改锁服务，让锁客户端只需要发送一次acquire RPC，把这个锁保存在缓存中，直到有其他的yfs_client需要再释放。&lt;/p&gt;

&lt;p&gt;这次的挑战需要修改客户端和服务器端的协议。例如当client2 需要某个被client1缓存在其本地的锁，需要服务通过revoke RPC revoke（找不出比较合适的词翻译） client1 的那个锁，返还给server。client2 才能够得到。&lt;/p&gt;

&lt;h2 id=&quot;id-getting-startedlab4&quot;&gt;Getting Started(Lab4)&lt;/h2&gt;
&lt;p&gt;需要改动的文件有：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;lock_client_cache.{cc,h}:这两个文件替换以前的lock_client文件，主要实现客户端缓存。&lt;/li&gt;
  &lt;li&gt;lock_server_cache.{cc,h}:同样的，代替以前的lock_server文件，实现对应的锁缓存服务&lt;/li&gt;
  &lt;li&gt;handle.{cc,h}:这个类主要包含了缓存节点之间的通信RPC，使用revoke和retry时使用。&lt;/li&gt;
  &lt;li&gt;tprintf.h:这个文件包含了一个宏，用来打印很多debug的信息，尤其是在分布式锁的调试中起到了很关键的作用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;id-step-one-design-the-protocollab4&quot;&gt;Step One: Design the Protocol(Lab4)&lt;/h2&gt;

&lt;p&gt;锁客户端需要对每一个锁记录其状态，并要有一个协议来表示他们目前的状态。设计这套协议，并思考协议怎样促使这些状态转换。
这是推荐的一套客户端协议：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;none      ：客户端不知道这个锁的存在&lt;/li&gt;
  &lt;li&gt;free      ：客户端拥有此锁并且也没有线程使用此锁&lt;/li&gt;
  &lt;li&gt;locked    ：客户端拥有此锁，并且有线程在使用此锁&lt;/li&gt;
  &lt;li&gt;aquiring  ：客户端正在请求此锁&lt;/li&gt;
  &lt;li&gt;releasing ：客户端正在释放此锁&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;状态转移图如图[1]所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/distribution_pic/state_machine.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[1] 整体系统结构。灰色圆圈中是每一个锁的状态。箭头上的时状态转移的条件。比如acq请求，release释放，revoke，retry 的远程调用等&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;为了满足这个记录客户端的这些信息，每个锁的结构需要重新规划。如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class ClientLock{
  public:
	pthread_cond_t wait_acq_;
	pthread_cond_t wait_free_;
	bool is_retried;
	bool is_revoked;
	bool is_finished;
	AcqRet::lock_status status;
	ClientLock():
	    is_retried(false),
	    is_revoked(false),
	    is_finished(false),
	    status(AcqRet::NONE){
	    pthread_cond_init(&amp;amp;wait_acq_, NULL);
	    pthread_cond_init(&amp;amp;wait_free_, NULL);
	}
};
class AcqRet {
public:
	enum status {OK = 0, RETRY};
	enum lock_status {NONE, FREE, LOCK, RELEASING, ACQING};
};

lock_protocol::status
lock_client_cache::acquire(lock_protocol::lockid_t lid)
{
  int ret = lock_protocol::OK;
  ScopeLock map_lock(&amp;amp;lock_map_mutex_);
  ClientLock &amp;amp;lock_item = m_lock_map_[lid];
  bool  is_used = false;
  while(!is_used){
    tprintf(&quot;lock_item.status is %d\n&quot;, lock_item.status);
	switch(lock_item.status)
	{
	  case AcqRet::NONE:
	      DealWithStatusNONE(lock_item, lid);
		  break;
	  case AcqRet::FREE:
	      DealWithStatusFREE(lock_item, lid);
	      is_used = true;
		  break;
	  case AcqRet::LOCK:
	      DealWithStatusLOCK(lock_item, lid);
		  break;
	  case AcqRet::ACQING:
	      DealWithStatusACQ(lock_item, lid);
		  break;
	  default:
		VERIFY(false);
	}
  }
  return ret;
}

void lock_client_cache::DealRelease(ClientLock &amp;amp; lock_item, lock_protocol::lockid_t lid)
{
    if (lock_item.status == AcqRet::NONE) return;
    if (lock_item.is_revoked){
        int r;
        lock_item.status = AcqRet::NONE;
        lock_item.is_finished = false;
        lock_item.is_revoked = false;
        pthread_mutex_unlock(&amp;amp;lock_map_mutex_);
        lu-&amp;gt;dorelease(lid);
        lock_protocol::status ret = cl-&amp;gt;call(lock_protocol::release, lid, id, r);
        tprintf(&quot;I send it %llu back to server\n&quot;, lid);
        VERIFY(ret == lock_protocol::OK);
        pthread_mutex_lock(&amp;amp;lock_map_mutex_);
    }else{
        lock_item.status = AcqRet::FREE;
        lock_item.is_finished = true;
    }
    pthread_cond_broadcast(&amp;amp;lock_item.wait_free_);
    pthread_cond_broadcast(&amp;amp;lock_item.wait_acq_);
}
void lock_client_cache::DealWithStatusACQ(ClientLock &amp;amp; lock_item, lock_protocol::lockid_t lid)
{
    if (lock_item.is_retried){
        SendAcqToSvr(lock_item, lid);
        tprintf(&quot;I am here retried 1\n&quot;);
    }
    while(lock_item.status == AcqRet::ACQING &amp;amp;&amp;amp; !lock_item.is_retried){
        tprintf(&quot;I am waiting here\n&quot;);
        pthread_cond_wait(&amp;amp;lock_item.wait_acq_, &amp;amp;lock_map_mutex_);
    }
    if (lock_item.is_retried &amp;amp;&amp;amp; lock_item.status == AcqRet::ACQING){
        SendAcqToSvr(lock_item, lid);
        tprintf(&quot;I am here retried 2\n&quot;);
    }
}
void lock_client_cache::DealWithStatusFREE(ClientLock &amp;amp; lock_item, lock_protocol::lockid_t lid)
{
    lock_item.is_finished = false;
     lock_item.status = AcqRet::LOCK;
}
void lock_client_cache::DealWithStatusLOCK(ClientLock &amp;amp; lock_item, lock_protocol::lockid_t lid)
{
    while(lock_item.status == AcqRet::LOCK){
        pthread_cond_wait(&amp;amp;lock_item.wait_free_, &amp;amp;lock_map_mutex_);
    }
}
void lock_client_cache::DealWithStatusNONE(ClientLock &amp;amp; lock_item, lock_protocol::lockid_t lid)
{

    SendAcqToSvr(lock_item, lid);
}

int lock_client_cache::SendAcqToSvr(ClientLock &amp;amp; lock_item, lock_protocol::lockid_t lid){
    int r;
    lock_item.is_retried  = false;
    pthread_mutex_unlock(&amp;amp;lock_map_mutex_);
    int ret = cl-&amp;gt;call(lock_protocol::acquire, lid, id, r);
    pthread_mutex_lock(&amp;amp;lock_map_mutex_);
    tprintf(&quot;I am send a msg to svr, ret=%d\n&quot;, ret);
    if (ret == AcqRet::OK){
        lock_item.status = AcqRet::FREE;
    } else if (ret) {
        lock_item.status = AcqRet::ACQING;
    } else {
        tprintf(&quot;unknow svr return %d\n&quot;, ret);
    }
    return ret;

}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;一个单独的客户端可能有多个线程等待相同的锁，但是每个客户端只有一个线程需要与server互动（interacting）。一旦某个有锁的线程释放掉此锁，那么就会唤醒那些等待这个所的其余线程。在等待的这些锁中，会有一个获得此锁。&lt;/p&gt;

&lt;p&gt;当一个client通过acquire RPC请求server时，如果此锁没有被任何client占有时，server返回 OK ，如果此锁被某个client占有时，返回RETRY。同时，server发送一个revoke给占有此锁的client，让他交出这个锁。当这个锁归还给server时，再向请求这个锁的client发出retry RPC，让它重新请求这个锁。如下图所示：&lt;/p&gt;

&lt;p&gt;一旦某个client获得了这个锁，client将会缓存此锁（cache，占有）。当他使用晚此锁时，不用release给server，如果这个client再需要acquire此锁时，就不需要向server请求了，从而减少了网络请求，提高了效率。知道有其他的client进行锁请求时，此client才（并且必须）交还此锁给server。&lt;/p&gt;

&lt;p&gt;服务器端应该思考怎样保存锁信息，出了是否locked这种信息，还需要保存比如每一个锁现在被谁占（cached），哪些clients正在等待此锁的释放等。&lt;/p&gt;

&lt;p&gt;服务端处理如图[2]所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/distribution_pic/lock_cache.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[2] 黑色为锁服务器在有缓存下的处理方式，红蓝为客户端，此图模拟了当锁遇到竞争时的状况，A先得锁，B等锁释放&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;struct CondLockCache : public CondLock{
	CondLockCache() {
	}
	~CondLockCache() {
	}
	std::string user_id_;       //lab4 add
};

//record which clints are waiting for this lock releasing
std::map&amp;lt;lock_protocol::lockid_t, std::queue&amp;lt;std::string&amp;gt; &amp;gt; m_retry_queue_; 

int lock_server_cache::acquire(lock_protocol::lockid_t lid, std::string id, 
                               int &amp;amp;r)
{
      AcqRet::status ret;
	  ScopeLock map_lock(&amp;amp;m_lock_mutex_);
	  CondLockCache&amp;amp; lock_item =  m_map_lock_[lid];
	  if (lock_item.is_locked == false){
		  lock_item.is_locked = true;
		  lock_item.user_id_ = id;
		  ret = AcqRet::OK;
	  }else{
		  VERIFY (!lock_item.user_id_.empty());
		  pthread_mutex_lock(&amp;amp;m_retry_mutex_);
		  m_retry_queue_[lid].push(id);
		  pthread_mutex_unlock(&amp;amp;m_retry_mutex_);
		  //ConnectToClient conn_c(lock_item.user_id_);
		  ret = AcqRet::RETRY;
		  pthread_mutex_unlock(&amp;amp;m_lock_mutex_);
		  int ret_r;
		  ret_r = handle(lock_item.user_id_).safebind()-&amp;gt;call(rlock_protocol::revoke, lid, r);
		  if (ret_r != rlock_protocol::OK) tprintf(&quot;rlock_protocol::revoke failed!!\n&quot;);
	  }
  return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在锁服务的acquire大致逻辑是，先判断此锁是否被锁，如果没有，直接返回OK，如果有，记录此客户端ID（IP：PORT）放入retry等待队列。随后像拥有此锁的客户端发送revoke，让其归还此锁。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;lock_protocol::status
lock_server_cache::release(lock_protocol::lockid_t lid, std::string id, 
         int &amp;amp;r)
{
  tprintf(&quot;%s send %llu to server\n&quot;, id.c_str(), lid);
  lock_protocol::status ret = lock_protocol::OK;
  ScopeLock map_lock(&amp;amp;m_lock_mutex_);
  if (m_map_lock_.count(lid) &amp;lt;= 0){
	  tprintf(&quot;lid is not acquired by anyone, why do you release?\n&quot;);
  }
  CondLockCache&amp;amp; lock_item =  m_map_lock_[lid];
  lock_item.is_locked = false;
  ScopeLock retry_lock(&amp;amp;m_retry_mutex_);
  std::queue&amp;lt;std::string&amp;gt; &amp;amp;rty_q = m_retry_queue_[lid];
  tprintf(&quot;now the retry queue size is %zu\n&quot;, rty_q.size());
  if (!rty_q.empty()){
      int ret_r;
	  std::string user_id = rty_q.front();
	  rty_q.pop();
	  //ConnectToClient conn_c(user_id);
	  pthread_mutex_unlock(&amp;amp;m_lock_mutex_);
	  pthread_mutex_unlock(&amp;amp;m_retry_mutex_);
	  ret_r = handle(user_id).safebind()-&amp;gt;call(rlock_protocol::retry, lid, r);
	  if (ret_r != rlock_protocol::OK) tprintf(&quot;rlock_protocol::retry failed!!\n&quot;);
	  pthread_mutex_lock(&amp;amp;m_lock_mutex_);
	  pthread_mutex_lock(&amp;amp;m_retry_mutex_);
	  if(!rty_q.empty()){
	    pthread_mutex_unlock(&amp;amp;m_lock_mutex_);
        pthread_mutex_unlock(&amp;amp;m_retry_mutex_);
        ret_r = handle(user_id).safebind()-&amp;gt;call(rlock_protocol::revoke, lid, r);
        if (ret_r != rlock_protocol::OK) tprintf(&quot;rlock_protocol::revoke failed!!\n&quot;);
        pthread_mutex_lock(&amp;amp;m_lock_mutex_);
        pthread_mutex_lock(&amp;amp;m_retry_mutex_);
	  }
  }
  return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;还锁时，查看等待队列，如果有等待发送retry给等待Client_X。这里需要注意：⚠️&lt;strong&gt;如果等待队列里还有等待此锁的客户端，要事先发送revoke给 Client_X，虽然它还没有拿到此锁，因为如果不这样，就再没有客户会请求此锁，也不会有revoke发送给Client_X&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里的user_id 直接记录了当前锁持有client的IP和port 同样在m_retry_queue_这个map中 queue中的string也是IP：PORT的形式，使用这样的方式第一能够保证唯一性，第二方便。
提示：当发送RPC时，要释放目前占有的锁，一个RPC可能维持的时间较长，如果你不希望其他的线程跟着一起等的话，就将它们释放掉。而且不释放还会导致分布式死锁。&lt;/p&gt;

&lt;p&gt;下列两个问题能够帮助你思考和设计：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;当一个client的线程保持一个锁，另一个线程发出acquire请求时，将会发生什么？这时时不会发送rpc请求的。&lt;/li&gt;
  &lt;li&gt;当一个客户端持有锁时，revoke请求应当怎样处理？当一个client在收到acquire的反馈之前收到了retry，该怎么办？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;提示：当一个client在收到acquire的反馈之前收到了retry，client应当记下这个请求，如果忽视了这个retry，如果你的acquire收到的是RETRY的话，client将永远陷入等待之中。（因为server不会再发送retry）&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;当一个client在收到acquire的反馈之前收到了revoke，该怎么办？记录起来（跟记录retry一样），返回OK，使用完这个锁后立即归还给server，不要做缓存。（因为server同样不会再发送revoke，此client以为没有其他clients需要，一直不肯归还，其他clients一直饥饿）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;id-introductionlab5&quot;&gt;Introduction(Lab5)&lt;/h2&gt;
&lt;p&gt;接下来要做的是对extents，也就是存储数据的服务实现缓存。原因与第一个锁缓存服务一样，都是为了减小开销，增加性能。最主要的任务就是要确保extents缓存下，某个client最终处理的某个数据，是上一个client在处理完后的结果。（不能在处理相同的数据时，各处理各的数据）。&lt;/p&gt;

&lt;p&gt;首先需要在client端添加一个本地的extent存储做cache。extent 客户端将再这个基础上对cache进行操作，同时客户端只有在没有某个extent并获取extent或者属性（attr）时访问extent服务器，并在其他extent客户端需要访问修改的时候将脏数据回写入extent服务器。与锁缓存服务有异曲同工之妙！&lt;/p&gt;

&lt;h2 id=&quot;id-step-oneextent-cachelab5&quot;&gt;Step One:Extent Cache(Lab5)&lt;/h2&gt;
&lt;p&gt;在第一步中，需要做一个带缓存的extent 客户端，不用考虑数据的一致性。首先新的extent_client::get应该检查客户端是否已经在本地的cache中，如果不在，访问extent服务器，将数据取回来，缓存在自己的数据库中。如果在，直接返回。put()函数中，直接替换cached中的数据，没有必要将已经修改的数据（脏数据）传给服务器，保存在缓存中。remove()应该删除本地的extent服务器。同样extent客户端应当记录一个数据的属性。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class FileData {
 public:
    FileContent file_content_;
    bool is_present;
    bool is_dirty;

    FileData():
        is_present(false),
        is_dirty(false)
    {}
};
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这是extent_client中extent的数据结构，除了以前的file content结构之外，添加了&lt;code&gt;is_present&lt;/code&gt;,&lt;code&gt;is_dirty&lt;/code&gt;两个标记。一个表示是否在本地cache中，一个表示已经修改，需要将脏数据回写入extent服务器。&lt;/p&gt;
&lt;h2 id=&quot;id-step-twocache-consistencylab5&quot;&gt;Step Two:Cache Consistency(Lab5)&lt;/h2&gt;
&lt;p&gt;在第二步中，要确保每一个get()请求请求到的数据时最近的put()改动过的数据，即使get与put的调用方时两个不同的extent客户端。extent客户端和锁服务相互合作以确保每一个inum（indoe）中的数据是一致的。当你释放锁的时候，要flush一下extent对应的inum数据，即回写入extent脏数据以更新extent服务器数据。（如果remove了，直接删了extent服务中对应的数据就好）&lt;/p&gt;

&lt;p&gt;假设clientA请求了锁inum，从extent服务器上get了对应的文件数据并在客户端自己的cache中修改了数据，当前clientA即缓存着锁服务的锁，也缓存着extent服务的数据。那么当clientB想要访问这份数据时，首先要拿到client缓存的锁，所以clientA要现将脏数据写入extent服务器中（返还这份数据），再释放对应的锁。这样clientB就能够看到clientA修改完后的数据了。&lt;/p&gt;

&lt;p&gt;这里还需要注意使用锁的两个目的：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;确保文件系统中的每一个操作的原子性&lt;/li&gt;
  &lt;li&gt;完成了extent缓存的一致性&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;确保每一个yfs_client对extent的操作包含在acquire和release请求中。&lt;/p&gt;

&lt;p&gt;YFS服务应当在释放锁到锁服务器上的时候再调用flush，而不是释放锁的时候。不要每一次调用lock_client::release()flush，而是要在lock_client调用&lt;code&gt;cl-&amp;gt;call(lock_protocol::release, lid, id, r);&lt;/code&gt;即真正返还锁给锁服务器时（锁服务向该client调用revoke时）再调用。否则就有些矫枉过正了。&lt;/p&gt;

&lt;p&gt;在这里，实验环境提供了lock_release_user类，这是一个虚类仅仅支持dorelease的成员方法。我们需要实现一个通过调用dorelease能够调用flush的子类。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;extent_protocol::status
extent_client_cache::get(extent_protocol::extentid_t eid, std::string &amp;amp;buf)
{
  extent_protocol::status ret = extent_protocol::OK;
  if (extent_file_cache_[eid].is_present){
      time_t now = time(0);
      extent_file_cache_[eid].file_content_.first.atime = static_cast&amp;lt;int&amp;gt;(now);
      buf = extent_file_cache_[eid].file_content_.second;
  }else{
    extent_protocol::attr &amp;amp;attr = extent_file_cache_[eid].file_content_.first;
    ret = cl-&amp;gt;call(extent_protocol::get, eid, buf);
    ret = cl-&amp;gt;call(extent_protocol::getattr, eid, attr);
    extent_file_cache_[eid].is_present = true;
    extent_file_cache_[eid].file_content_.second = buf;
  }
  return ret;
}

extent_protocol::status
extent_client_cache::getattr(extent_protocol::extentid_t eid,
               extent_protocol::attr &amp;amp;attr)
{
  extent_protocol::status ret = extent_protocol::OK;
  if (extent_file_cache_[eid].is_present){
      attr = extent_file_cache_[eid].file_content_.first;
  } else {
      ret = cl-&amp;gt;call(extent_protocol::getattr, eid, attr);
  }
  return ret;
}

extent_protocol::status
extent_client_cache::put(extent_protocol::extentid_t eid, std::string buf)
{
  extent_protocol::status ret = extent_protocol::OK;
  time_t now = time(0);

  std::map&amp;lt;uint64_t, FileData&amp;gt;::iterator file_cache_it = extent_file_cache_.find(eid);
  if (file_cache_it == extent_file_cache_.end()){
      extent_file_cache_[eid].file_content_.first.atime = static_cast&amp;lt;int&amp;gt;(now);
  }
  extent_file_cache_[eid].file_content_.first.mtime = static_cast&amp;lt;int&amp;gt;(now);
  extent_file_cache_[eid].file_content_.first.ctime = static_cast&amp;lt;int&amp;gt;(now);
  //int r;
  //ret = cl-&amp;gt;call(extent_protocol::put, eid, buf, r);
  extent_file_cache_[eid].file_content_.first.size = buf.size();
  extent_file_cache_[eid].file_content_.second = buf;
  extent_file_cache_[eid].is_dirty = true;
  extent_file_cache_[eid].is_present = true;
  return ret;
}

extent_protocol::status
extent_client_cache::remove(extent_protocol::extentid_t eid)
{
  extent_protocol::status ret = extent_protocol::OK;
  std::map&amp;lt;uint64_t, FileData&amp;gt;::iterator file_cache_it = extent_file_cache_.find(eid);
  if (file_cache_it != extent_file_cache_.end()){
      extent_file_cache_.erase(file_cache_it);
  }else{
      int r;
      ret = cl-&amp;gt;call(extent_protocol::remove, eid, r);
  }

  return ret;
}

extent_protocol::status extent_client_cache::flush(extent_protocol::extentid_t eid)
{
    extent_protocol::status ret = extent_protocol::OK;
    std::map&amp;lt;uint64_t, FileData&amp;gt;::iterator file_cache_it = extent_file_cache_.find(eid);
    if (file_cache_it == extent_file_cache_.end()){
        int r;
        ret = cl-&amp;gt;call(extent_protocol::remove, eid, r);
        return ret;
    }

    if (file_cache_it-&amp;gt;second.is_dirty){
        int r;
        std::string &amp;amp;buf = file_cache_it-&amp;gt;second.file_content_.second;
        ret = cl-&amp;gt;call(extent_protocol::put, eid, buf, r);
        tprintf(&quot;put %d back to server, ret=%d\n&quot;, eid, ret);
    }
    extent_file_cache_[eid].is_dirty   = false;
    extent_file_cache_[eid].is_present = false;
    return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在缓存实验后，我们可以打开rpc的记录日志（在环境中设置 &lt;code&gt;
export RPC_COUNT=25&lt;/code&gt;
）,它会记录所有rpc请求的数量。在通过执行test-lab-3-c,可以观察在加缓存和没有缓存情况下，客户端请求服务器的次数。实验手册上说如果设计得当，锁服务的请求会降低10倍（很惭愧，只降低了3倍）。其他的请求次数也会有大幅度下降。图[3]为实验对比：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/distribution_pic/cache_perform.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[3]acq没有试验手册下降的多，release到是下降了10倍左右，在extent的试验中，Put和remove下降最大，Get与GetAttr下降明显。GetInum因为没有做缓存，而且在我做实验中设计的不好，随机生成，不太方便做缓存，所以也就没下降&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;id-hints&quot;&gt;Hints&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;确保使用线程mutex来保护客户端的extent cache，防止多线程资源竞争。&lt;/li&gt;
  &lt;li&gt;如果你对extent中的某个data只是做了只读（read-only）的操作，久不要将这一数据flush入extent服务器了，应该使用一个标记位来记录数据是否被修改（&lt;code&gt;is_dirty&lt;/code&gt;）&lt;/li&gt;
  &lt;li&gt;这个实现可以不怎么修改yfs_client.cc这一文件，可以通过继承client_extent的方式，实现extent_client_cache，重新定义put get等方式。（因为yfs_client调用extent的方式是指针，这一点实验设计的很巧妙）&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  <entry>
    <title>6.824:Lab2-3 FileSystem</title>
    <link href="http://localhost:4000//2018/05/22/6.824-Lab2-3-FileSystem.html"/>
    <updated>2018-05-22T00:00:00+08:00</updated>
    <id>//2018/05/22/6.824:Lab2-3-FileSystem</id>
    <content type="html">&lt;h1 id=&quot;id-lab2-3-file-server&quot;&gt;Lab2-3 File Server&lt;/h1&gt;
&lt;h2 id=&quot;id-introductionlab2&quot;&gt;Introduction(Lab2)&lt;/h2&gt;
&lt;p&gt;在这个实验中，将使用FUSE的接口完成一个文件系统&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CREATE／MKNOD， LOOKUP， 和 READDIR&lt;/li&gt;
  &lt;li&gt;SETTATTR，WRITE，READ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些操作是完成一个文件系统的最根本的几个操作之一。使用下图的架构。&lt;/p&gt;

&lt;p&gt;我们将提供YFS和extent服务器模块的基本框架,如图[1]所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/distribution_pic/system_architecture.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[1] 整体系统结构&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;YFS模块时实现文件系统基本核心逻辑的模块，这一模块最后被编译为一个yfs_client支持本地挂载的文件系统。这一模块的代码框架由两部分组成：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;FUSE 接口&lt;/strong&gt;，在这个fuse.cc中，这个代码将FUSE操作从内核模块变为了YFS客户端调用。实验已经提供给了FUSE内核的注册操作，需要做的是，修改fuse.cc使其能够正确地调用yfs_client提供的成员方法，再通过FUSE接口返回。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;YFS 客户端&lt;/strong&gt;，在yfs_client.{cc,h}。YFS clients使用extent和lock服务实现了分布式文件系统的主要逻辑。例如当创建一个新的文件时，你的yfs_client就要在目录数据中增加一个目录项（item）在extent服务器中，这一模块是调用方，即为客户端，而不是想文件服务器，活着时锁服务器一样的服务器。（fuse-&amp;gt;yfs_client-&amp;gt;extent client-&amp;gt; extent server），如图[2]所示。 
  &lt;img src=&quot;/images/distribution_pic/yfs_client.png&quot; alt=&quot;image&quot; /&gt;
&lt;em&gt;如图[2] 整体客户端结构，exe代表操作系统上运行的可执行文件，比如touch，ls等&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;extent服务器存储着文件系统所有的数据，像是在二进制文件中的硬盘的作用。在随后的实验中，你将在多个主机上运行YFSclient，他们都使用一个公共的extent服务，这也就意味着所有的主机使用着和共享着相同的数据。extent服务器代码框架包含有两个方面：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Extent客户端&lt;/strong&gt; 在extent_client.{cc.h}封装了与extent服务器通行的RPC。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Extent服务器&lt;/strong&gt; 在extent_server.{cc.h}和extent_smain，extent服务器管理了一个简单的K-V存储，它使用extent_protocol::extentid_t作为Keys，V包含两个方面，使用string作为存储的数据类型，使用一个结构体包换这个数据的属性（attr）。服务器包含四个方法：put(k,v), v_s get(k), v_a getattr(k), remove(k).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;id-getting-started&quot;&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;首先给电脑的操作系统安装FUSE库（详见&lt;a href=&quot;https://pdos.csail.mit.edu/archive/6.824-2012/labs/index.html&quot;&gt;实验概述&lt;/a&gt;）&lt;/p&gt;

&lt;p&gt;框架代码仅仅实现了GETATTR和STATFS这两个接口，所以刚开始挂载的文件系统是不能够正常使用的。实验将完善文件系统中更多的FUSE的操作&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;第一部分：应当完成CREATE/MKNOD,LOOKUP,和READDIR。代码应当通过test-lab-2-a.pl这一脚本。这个脚本主要的测试项是创建空文件，在目录中查询文件名，列出目录中的文件内容。&lt;/li&gt;
  &lt;li&gt;第二部分：需要实现SETATTR，READ和WRITE，你的代码应当通过test-lab-2-b.pl这一脚本。这个脚本的主要测试读，写追加文件。还测试文件是否能够在一个客户端创建，在另一个客户端读取。
确保你在Linux操作的系统的用户在FUSE的用户组中，这样才能够操作FUSE格式的文件系统。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;id-part1createmknod-lookup-and-readdir&quot;&gt;PART1:CREATE/MKNOD, LOOKUP, and READDIR&lt;/h2&gt;
&lt;h3 id=&quot;id-your-job&quot;&gt;your job&lt;/h3&gt;
&lt;p&gt;在第一部分的工作是实现一个extent服务（存储服务），然后实现CREATE NKNOD，LOOKUP和READDIR 的FUSE文件操作。必须使用extent服务器来存储文件系统的内容，这样才能够完成part2中共享数据以及后续的实验。当然可以将extent的存储放在内存中（我就只使用了c++ 的map），只是一旦关闭extent服务器，所有的数据就都丢失了。&lt;/p&gt;

&lt;p&gt;在一些系统中，FUSE使用MKNOD创建文件，在另一些系统中，使用CREATE。这两个接口有一些小区别，但是为了方便，我们提供了一个createhelper()接口，MKNOD或者CREATE接口都wrap了这个接口。实验所要做的工作就是实现createhelper()方法。&lt;/p&gt;

&lt;h3 id=&quot;id-step-one&quot;&gt;Step One&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;实现extent服务器：需要再extent_server.cc中实现这个服务。一共有四个操作put(k,v), get(k), getattr(k)和remove(k)。put和getRPC被用作更新和获取数据（extent的content）。getattrRPC获取数据的“属性”，属性包含了数据最后修改时间（mtime），最后改变属性的时间（ctime）和最后获取数据的时间（atime）。时间以秒作为单位从1970年起计算（unix时间戳）。attribute有点像unix系统的i-node。你可以使用转本的attribue存数，不用像inode一样将元数据和数据都存储在里边。当调用put()时，ctime和mtime一样改变。调用get()时，atime会改变。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;文件/目录 ID&lt;/p&gt;

    &lt;p&gt;YFS和FUSE都需要一个能够辨识唯一的标识符，如果UNIX文件系统中的inode标号。FUSE使用32-bit作为标识符，你的YFS应当使用64-bit的数字作为标识符，其中高32位为0，低32位作为文件/目录的ID，在这个系统里，标识符在yfs_client.h成为inum。&lt;/p&gt;

    &lt;p&gt;当创建一个新的文件（使用fuseserver_createhelper）或者文件（fuseserver_mkdir），你不必分配一个inum，简单点地做法就是随机取一个数组，只要不重复就可以但要考虑当文件和目录不断增加时，冲突的概率。（我刚开始想学unix做bitmap，后来做到客户端了。。。所以这里偷了个懒，就使用随机数生成，使用map count作为判断是否冲突。）&lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;  int extent_server::GetInodeNum(int is_file, extent_protocol::extentid_t&amp;amp; rino){
  	extent_lock ex_lc(&amp;amp;pmutex_file_block_map_);
  	extent_protocol::extentid_t ino;
  	struct timeval tv;
  	gettimeofday(&amp;amp;tv,NULL);
  	srand(tv.tv_usec);
  	do{
  		ino = rand() % 0x80000000LLU;
  		if (ino == 1)
  			continue;
  		if(is_file){
  			ino |= 0x80000000LLU;
  		}
  	}while(file_block_.count(ino) &amp;gt; 0);
  	rino = ino;
  	return extent_protocol::OK;
  }
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;YFS需要通过inum来区分哪一个是文件数据，哪一个是目录数据。这里使用31bit作为inum号进行分配。这里使用&lt;code&gt;yfs_client::isfile&lt;/code&gt;来判断，如果第31位为0，为目录，1为文件。&lt;/p&gt;
    &lt;pre&gt;&lt;code&gt;  bool
  yfs_client::isfile(inum inum)
  {
    if(inum &amp;amp; 0x80000000)
      return true;
    return false;
  }
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;目录格式：&lt;/p&gt;

    &lt;p&gt;下一个任务就是定义目录的存储结构格式。一个目录数据应当包含目录中名称和这对应inum的映射。当然存储的时候，需要把目录数据转换成一个string存储在extent服务器中。LOOKUP和READDIR读取目录内容，CREATE/MKNOD修改目录内容。以下是我对目录的定义：&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  /*
      *dir_block_：map&amp;lt;FILE_NAME, INUM&amp;gt;
      *dir_item_num_ :目录项个数
      *StringToDirForm 将extent的数据 转换成目录结构数据
      *DirFormToString 将目录结构数据 转化成extent数据
  */
  struct DirForm{
  	std::map&amp;lt;std::string, uint64_t&amp;gt; dir_block_;
  	int dir_item_num_;
  	void StringToDirForm(const std::string&amp;amp;);
  	void DirFormToString(std::string&amp;amp;);
  };
  void DirForm::StringToDirForm(const std::string&amp;amp; data){
  	const char* start = data.data();
  	std::size_t pos = 0;
  	dir_item_num_ = *(int *) (start + pos);
  	printf(&quot;dir_item_num_ = %d\n&quot;, dir_item_num_);
  	pos += sizeof(int);
  	for(int i = 0; i &amp;lt; dir_item_num_ &amp;amp;&amp;amp; pos &amp;lt; data.size(); i++) {
  		std::string item_name;
  		item_name.assign((char *) (start + pos), MAX_ITEM_NAME_LEN);
  		pos += MAX_ITEM_NAME_LEN;
  		uint64_t item_num = *(uint64_t *)(start + pos);
  		pos += sizeof(uint64_t);
  		dir_block_.insert(std::make_pair(item_name, item_num));
  	}
  }

  void DirForm::DirFormToString(std::string&amp;amp; data){
  	data.clear();
  	dir_item_num_ = dir_block_.size();
  	char a32[4];
  	memcpy(a32, &amp;amp;dir_item_num_, 4);
  	data.append(a32, sizeof(dir_item_num_));
  	typeof(dir_block_.begin()) mit = dir_block_.begin();
  	for(;mit != dir_block_.end(); mit++){
  		std::string tmp_str = mit-&amp;gt;first;
  		tmp_str.resize(MAX_ITEM_NAME_LEN);
  		data.append(tmp_str.c_str(), MAX_ITEM_NAME_LEN);
  		char a64[8];
  		memcpy(a64, &amp;amp;(mit-&amp;gt;second), 8);
  		data.append(a64, sizeof(mit-&amp;gt;second));
  	}
  }
&lt;/code&gt;&lt;/pre&gt;
    &lt;p&gt;这里就不需要考虑文件所有者，模式或者是权限等。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;FUSE：&lt;/p&gt;

    &lt;p&gt;当一个程序（像ls或者文本编辑器）在你的yfs_client中操作一个文件或者是目录时，在内核中的代码通过FUSE传递给yfs_client。实验已在fuse.cc中实现了这一接口，比如create，read，write等等操作。你应当在fuse.cc中修改相关的接口，fuse接口中的create调用yfs_client中的create。这个基本的操作可以参考已经实现的gettattr函数。&lt;/p&gt;

    &lt;p&gt;相关的一些方法可以在fuse_lowlevel.h中进行详细的理解，比如FUSE使用fuse_reply把成功的结果返回给内核，通过fuse_reply_err报告一个错误。&lt;/p&gt;

    &lt;p&gt;在通过READDIR读取目录信息的时候，使用了一些trick，我们已经实现了dirbuf_add,reply_buf_limited等方法，你所要做的就是修改FUSE中的READDIR让系统获得这个目录下的目录项。&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;尽管当你创建新的文件或是目录时，可以随意的使用inum作为标识符，但是FUSE是将0x00000001作为根目录（root）的。因此你应当确保0x00000001这个不会作为普通文件或者目录的inum，因为它已被占用&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;YFS code：&lt;/p&gt;

    &lt;p&gt;文件系统的主要逻辑应当在yfs_client中完成，因为大部分的操作都是fuse调用yfs_client。fuse只是作为一个handle。在yfs_client中，使用get(inum)从extent服务器获取数据。在目录文件中，要能够将string解析成文件目录结构。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;tips&lt;/p&gt;

    &lt;p&gt;如果一个文件已经存在（filename在相同的目录下重复），CREATE操作就应当返回EEXIST给FUSE。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;//创建文件或目录
yfs_client::status
fuseserver_createhelper(fuse_ino_t parent, const char *name,
                        mode_t mode, struct fuse_entry_param *e)
{
  // In yfs, timeouts are always set to 0.0, and generations are always set to 0
  e-&amp;gt;attr_timeout = 0.0;
  e-&amp;gt;entry_timeout = 0.0;
  e-&amp;gt;generation = 0;
  // You fill this in for Lab 2
  uint64_t inum;
  int ret = yfs-&amp;gt;create(parent, name, inum);
  if (ret != yfs_client::OK) {
	  return ret;
  }
  e-&amp;gt;ino = inum;
  getattr(inum, e-&amp;gt;attr);
  printf(&quot;fuseserver_createhelper create item succ!\n&quot;);
  return yfs_client::OK;

}

yfs_client::status yfs_client::create(uint64_t parent, const char *name, uint64_t&amp;amp; ret_ino){
	  yfs_client::status ret;
	  std::string data;
	  std::string str_name = name;
	  str_name.resize(MAX_ITEM_NAME_LEN);
	  printf(&quot;parent id: %lu\n&quot;, parent);
	  printf(&quot;file name is %s\n&quot;, str_name.c_str());
	  YfsLock yl(lc, parent);
	  ret = getdata(parent, data);
	  if (ret != yfs_client::OK){
		  return yfs_client::NOENT;
	  }
	  DirForm dir_info;
	  dir_info.StringToDirForm(data);
	  if (dir_info.dir_block_.count(str_name) &amp;gt; 0) {
		  return yfs_client::EXIST;
	  }
	  ret_ino = GetAvailInum(true);
	  printf(&quot;fuseserver_createhelper ino:%lu\n&quot;, ret_ino);
	  if (ret_ino == 0) {
		  return yfs_client::NOENT;
	  }
	  dir_info.dir_block_[str_name] = ret_ino;
	  dir_info.DirFormToString(data);
	  ret = putdata(parent, data);
	  if (ret != yfs_client::OK) {
		  RestoreInum(ret_ino, true);
		  return ret;
	  }
	  data.clear();
	  ret = putdata(ret_ino, data);
	  if (ret != yfs_client::OK){
		  RestoreInum(ret_ino, true);
		  return ret;
	  }
	  yfs_client::fileinfo info;
	  ret = getfile(ret_ino, info);
	  if (ret != yfs_client::OK){
	  	  RestoreInum(ret_ino, true);
	  	  return ret;
	  }
	  return yfs_client::OK;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;//LOOKUP操作
void
fuseserver_lookup(fuse_req_t req, fuse_ino_t parent, const char *name)
{
  struct fuse_entry_param e;
  // In yfs, timeouts are always set to 0.0, and generations are always set to 0
  e.attr_timeout = 0.0;
  e.entry_timeout = 0.0;
  e.generation = 0;
  bool found = false;

  // You fill this in for Lab 2
  yfs_client::status ret = yfs-&amp;gt;lookup(parent, name, found, e.ino);
  if (ret != yfs_client::OK)
	  goto lookup_end;

  if(found){
	  getattr(e.ino, e.attr);
  }
 lookup_end:

  if (found)
    fuse_reply_entry(req, &amp;amp;e);
  else
    fuse_reply_err(req, ENOENT);
}
yfs_client::status yfs_client::lookup(uint64_t parent, const char *name, bool&amp;amp; found, uint64_t&amp;amp; imun){
	  DirForm dirinfo;
	  std::string data;
	  std::string str_name = name;
	  str_name.resize(MAX_ITEM_NAME_LEN);
	  printf(&quot;input name=%s\n&quot;, str_name.c_str());
	  std::map&amp;lt;std::string, uint64_t&amp;gt;::iterator mit;
	  YfsLock yl(lc, parent);
	  if(getdata(parent, data) != yfs_client::OK){
		  return yfs_client::NOENT;
	  }

	  dirinfo.StringToDirForm(data);
	  printf(&quot;fuseserver_lookup dirinfo.dir_block_ size is %lu\n&quot;, dirinfo.dir_block_.size());
	  mit = dirinfo.dir_block_.begin();
	  /*
	  for(;mit != dirinfo.dir_block_.end(); mit++) {
		  printf(&quot;name:%s,in:%llu\n&quot;,mit-&amp;gt;first.c_str(), mit-&amp;gt;second);

	  }
	  */
	  if (dirinfo.dir_block_.count(str_name) &amp;gt; 0){
		  imun = dirinfo.dir_block_[str_name];
		  found = true;
	  }
	  return yfs_client::OK;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;\\READDIR
void
fuseserver_readdir(fuse_req_t req, fuse_ino_t ino, size_t size,
                   off_t off, struct fuse_file_info *fi)
{
  yfs_client::inum inum = ino; // req-&amp;gt;in.h.nodeid;
  struct dirbuf b;

  printf(&quot;fuseserver_readdir\n&quot;);

  if(!yfs-&amp;gt;isdir(inum)){
    fuse_reply_err(req, ENOTDIR);
    return;
  }

  memset(&amp;amp;b, 0, sizeof(b));


  // You fill this in for Lab 2
  DirForm dirinfo;
  yfs_client::status ret = yfs-&amp;gt;readdir(dirinfo, ino);
  if (ret != yfs_client::OK){
	  printf(&quot;readdir:yfs-&amp;gt;readdir return %d&quot;, ret);
  }
  typeof(dirinfo.dir_block_.begin()) mit = dirinfo.dir_block_.begin();
  for(;mit != dirinfo.dir_block_.end(); mit++){
	  dirbuf_add(&amp;amp;b, mit-&amp;gt;first.c_str(), mit-&amp;gt;second);
  }
  reply_buf_limited(req, b.p, b.size, off, size);
  free(b.p);
}
yfs_client::status yfs_client::readdir(DirForm&amp;amp; dirinfo, uint64_t inum){
	;
	std::string data;
	YfsLock yl(lc, inum);
	yfs_client::status ret = getdata(inum, data);
	if (ret != yfs_client::OK){
		return ret;
	}
	  dirinfo.StringToDirForm(data);
	  return yfs_client::OK;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;id-part2-settattrreadwritelab2&quot;&gt;PART2 SETTATTR，READ，WRITE（LAB2）&lt;/h2&gt;
&lt;h3 id=&quot;id-part-2-your-jobs&quot;&gt;Part 2 Your Jobs&lt;/h3&gt;
&lt;p&gt;在Part中需要实现SETATTR，WRITE，READ等FUSE操作，只要通过了test-lab-2-b.pl Lab2就算是完成了。test-lab-2-b.pl测试了读、写、追加文件，在一个客户端写，查看另一个客户端是否可读。&lt;/p&gt;

&lt;h3 id=&quot;id-part2detailed-guidance&quot;&gt;Part2：Detailed Guidance&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;实现SETATTR&lt;/p&gt;

    &lt;p&gt;操作系统通过FUSE的SETATTR操作告诉你的文件系统设置文件属性。在to_set参数中设置哪一个需要被设置。目前只有一个文件属性需要注意就是文件大小的属性。设置FUSE_SET_ATTR_SIZE，操作系统也许会通过覆盖写，创建一个已存在的文件。那么就不会通调用FUSE的CREATE，而是SETATTR改变文件size属性。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;实现READ\WRITE&lt;/p&gt;

    &lt;p&gt;READ（fuseserver_read）读一个文件，需要一个读取文件大小的size和从哪里开始的offset作为参数。当size小于文件size时，当读取数据超出了文件范围时，能读多少返回多少可读的数据。在linux系统中使用man 查看read的详细信息。&lt;/p&gt;

    &lt;p&gt;对于WRITE（fuseserver_write）来说，需要一个写入数据大小的size和从哪里开始的offset作为、数据这三个参数。当offset值大于文件原本size的时候，大于的部分全部填充‘\0’。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;yfs_client::status yfs_client::read(uint64_t ino, size_t size, off_t off, std::string &amp;amp;buf){
	  std::string data;
	  YfsLock yl(lc, ino);
	  yfs_client::status ret = getdata(ino, data);
	  if (ret != yfs_client::OK) {
		  return yfs_client::NOENT;
	  }
	  if ((uint64_t)off &amp;gt;= data.size()) {
		  buf = &quot;&quot;;
	  }else if ((off + size) &amp;gt; data.size()){
		  buf = data.substr(off);
	  }else{
		  buf = data.substr(off, size);
	  }
	  return yfs_client::OK;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;yfs_client::status yfs_client::write(uint64_t ino, size_t size, off_t off, const char* buf){
	  std::string data;
	  std::string write_data;
	  YfsLock yl(lc, ino);
	  yfs_client::status ret = getdata(ino, data);
	  if (ret != yfs_client::OK) {
		  return ret;
	  }
	  if ((uint64_t)off &amp;gt; data.size()) {
		  data.resize(off);
	  }
	  write_data = data.substr(0, off);
	  if ((off + size) &amp;gt; data.size()){
		  write_data.append(buf, size);
	  } else {
		  std::string tmp_str(buf, size);
		  write_data += tmp_str;
		  write_data += data.substr(off + size);
	  }
	  ret = putdata(ino, write_data);
	  if (ret != yfs_client::OK){
		  return ret;
	  }
	  return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;yfs_client::status yfs_client::setattr(uint64_t ino, struct stat *attr){
	  std::string data;
	  YfsLock yl(lc, ino);
	  yfs_client::status ret = getdata(ino, data);
	  if (ret != yfs_client::OK) {
		  return ret;
	  }
	  data.resize(attr-&amp;gt;st_size);
	  ret = putdata(ino, data);
	  if (ret != yfs_client::OK) {
		  return ret;
	  }
	  return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;id-lab3mkdirunlinkand-locking&quot;&gt;Lab3：MKDIR,UNLINK,and Locking&lt;/h2&gt;

&lt;h3 id=&quot;id-introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;在这个实验中，&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在FUSE中增加MKDIR和UNLINK操作&lt;/li&gt;
  &lt;li&gt;在yfs_client中加入锁操作，以确保在同一个目录下，不同用户之间操作不会有冲突&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;id-part1mkdirunlink&quot;&gt;Part1：MKDIR,UNLINK&lt;/h3&gt;

&lt;h4 id=&quot;id-your-job-1&quot;&gt;Your Job&lt;/h4&gt;
&lt;p&gt;这个任务主要是在FUSE中实现MKDIR和UNLINK，确保在MKDIR创建是，标志文件与目录的inum的那个bit位为0。对于MKDIR，你不用在创建每个目录的时候再创建“.”，“..”因为Linux内核对于YFS是透明的。UNLINK只需要删除对应inum的extent即可，不需要实现UNIX系统那样的文件应用计数。&lt;/p&gt;

&lt;p&gt;如果全部通过test-lab-3-a.pl脚本测试，那么就算是完成了part1的内容了。这个测试脚本包含了创建目录，创建删除文件，检查目录extent的mtime和ctime属性。要注意的是测试脚本会检查时间属性的正确性。创建一个文件会同时改变父文件的mtime和ctime。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fuseserver_mkdir(fuse_req_t req, fuse_ino_t parent, const char *name,
     mode_t mode)
{
  struct fuse_entry_param e;
  // In yfs, timeouts are always set to 0.0, and generations are always set to 0
  e.attr_timeout = 0.0;
  e.entry_timeout = 0.0;
  e.generation = 0;
  // Suppress compiler warning of unused e.
  //(void) e;
  // You fill this in for Lab 3
#if 1
  yfs_client::status ret = yfs-&amp;gt;mkdir(parent, name, e.ino);
  if (ret == yfs_client::EXIST) {
	fuse_reply_err(req, EEXIST);
	return;
  }

  if (ret != yfs_client::OK) {
	fuse_reply_err(req, ENOENT);
	return;
  }

  ret = getattr(e.ino, e.attr);

  if (ret != yfs_client::OK) {
	fuse_reply_err(req, ENOENT);
	return;
  }

  fuse_reply_entry(req, &amp;amp;e);
#else
  fuse_reply_err(req, ENOSYS);
#endif
}

yfs_client::status yfs_client::mkdir(uint64_t parent, const char *name, uint64_t&amp;amp; ret_ino) {
	  yfs_client::status ret;
	  std::string data;
	  std::string str_name = name;
	  str_name.resize(MAX_ITEM_NAME_LEN);
	  printf(&quot;parent id: %lu\n&quot;, parent);
	  printf(&quot;file name is %s\n&quot;, str_name.c_str());
	  YfsLock yl(lc, parent);
	  ret = getdata(parent, data);
	  if (ret != yfs_client::OK){
		  return yfs_client::NOENT;
	  }
	  DirForm dir_info;
	  dir_info.StringToDirForm(data);
	  if (dir_info.dir_block_.count(str_name) &amp;gt; 0) {
		  return yfs_client::EXIST;
	  }
	  ret_ino = GetAvailInum(false);
	  printf(&quot;fuseserver_createhelper ino:%lu\n&quot;, ret_ino);
	  if (ret_ino == 0) {
		  return yfs_client::NOENT;
	  }
	  dir_info.dir_block_[str_name] = ret_ino;
	  dir_info.DirFormToString(data);
	  ret = putdata(parent, data);
	  if (ret != yfs_client::OK) {
		  RestoreInum(ret_ino, false);
		  return ret;
	  }
	  data.clear();
	  ret = putdata(ret_ino, data);
	  if (ret != yfs_client::OK){
		  RestoreInum(ret_ino, false);
		  return ret;
	  }
	  yfs_client::fileinfo info;
	  ret = getfile(ret_ino, info);
	  if (ret != yfs_client::OK){
		  RestoreInum(ret_ino, false);
		  return ret;
	  }
	  return yfs_client::OK;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;void
fuseserver_unlink(fuse_req_t req, fuse_ino_t parent, const char *name)
{

  // You fill this in for Lab 3
  // Success:	fuse_reply_err(req, 0);
  // Not found:	fuse_reply_err(req, ENOENT);

  yfs_client::status ret = yfs-&amp;gt;unlink(parent, name);
  if (ret != yfs_client::OK) {
	fuse_reply_err(req, ENOENT);
	return;
  }

  fuse_reply_err(req, 0);
  return;

  fuse_reply_err(req, ENOSYS);
}

yfs_client::status yfs_client::unlink(uint64_t parent, const char *name){
	  yfs_client::status ret;
	  std::string data;
	  std::string str_name = name;
	  str_name.resize(MAX_ITEM_NAME_LEN);
	  YfsLock yl(lc, parent);
	  ret = getdata(parent, data);
	  if (ret != yfs_client::OK){
		  return yfs_client::NOENT;
	  }
	  DirForm dir_info;
	  dir_info.StringToDirForm(data);
	  std::map&amp;lt;std::string, uint64_t&amp;gt;::iterator del_mit = dir_info.dir_block_.find(str_name);
	  //printf(&quot;unlink str-name:%s\n, parent is %llu\n&quot;, str_name.c_str(), parent);
	  if (del_mit == dir_info.dir_block_.end()) {
		  return yfs_client::NOENT;
	  }
	  uint64_t ino = del_mit-&amp;gt;second;
	  //printf(&quot;yfs_client::unlink ino:%ullX\n&quot;, ino);
	  if (!isfile(ino)) {
		  return yfs_client::NOENT;
	  }
	  /*
	  int ret_restore = RestoreInum(true, ino);

	  if (ret_restore != 0) {
		  return yfs_client::NOENT;
	  }
	  */
	  dir_info.dir_block_.erase(del_mit);
	  dir_info.DirFormToString(data);
	  ret = putdata(parent, data);
	  if (ret != yfs_client::OK) {
		  return ret;
	  }
	  YfsLock yl2(lc, ino);
	  if (ec-&amp;gt;remove(ino) != extent_protocol::OK) {
		return yfs_client::NOENT;
	  }

	return yfs_client::OK;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;id-part-2locking&quot;&gt;Part 2：Locking&lt;/h3&gt;

&lt;p&gt;下一步就要考虑当有多个yfs_client客户端运行的时候，保证文件操作的原子性了。现在如果yfs_client 的create方法会调用extent服务器中父目录的一些内容，做一些改变，并存储一些新的东西给extent服务器。在两个客户端同时运行的时候，会出点两个客户端同时获取一个extent服务器的旧数据的信息，每一个会分别插入一个新的内容在这个目录文件中，然后写入extent服务器中。那么最终这个extent中的数据是最后一个人新加的东西。当然正确的结果是这两个客户添加的东西都要加上，这是一个潜在的竞争。在并发CREATE UNLIN，并发 MKDIR UNLINK，并发WRITE中都会存在。&lt;/p&gt;

&lt;p&gt;所以，可以要使用锁服务来消除竞争。比如yfs_client应当在CREATE的时候请求一个锁，当修改完毕后，在把这个锁释放掉。如果存在并发操作，这个锁会将并发的两个操作分开，保证原子性。左右yfs_client必须在相同的一个锁服务器上请求。逻辑如图[3]所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/distribution_pic/system_logic.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;图[3]&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;id-your-job-2&quot;&gt;Your Job&lt;/h4&gt;
&lt;p&gt;主要任务就时在yfs_client操作的时候加上锁来确保并发的正确性。test-lab-3-b和test-lab-3-c用作测试。如果你在添加锁之前见进行了测试，那么会在并发创建（concurrent create）。如果在添加了锁之后还有这个错误，那就是有bug了。&lt;/p&gt;

&lt;h4 id=&quot;id-detailed-guidance&quot;&gt;Detailed Guidance&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;关于锁&lt;/p&gt;

    &lt;p&gt;最极端的例子就是，整个文件系统中只是用一个锁，这样所有的操作都不会是并行的。还有一个比较极端的例子就是在一个目录下使用一个锁，当然这也不是一个好的方法。单一个全局变量的锁就保证了所有的并发，但是一个细粒度的锁就会增加开销，而且容易发生死锁，因为可能在某些操作时，会需要持有多个锁。&lt;/p&gt;

    &lt;p&gt;应当给每一个锁一个inum。这样在对文件或者目录做操作的时候就会比较容易，不用再做什么转换。对那个inum操作，申请哪个inum的锁，使用完后直接释放就好。当然在发生了error，也要及时的释放锁。&lt;/p&gt;

    &lt;p&gt;在yfs_client中使用lab1中实现的lock服务就好，就像使用extent_client一样方便。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;注意以下事件&lt;/p&gt;

    &lt;p&gt;这是首次挂载两个YFS，如果在以前的实验中没有注意的话你可能在分配inum时会出现一些问题，即分配了两个相同的inum。有一种避免的方法就是使用随机数生成inum，可以通过pid生成。（我的代码使用的是时间毫秒级，秒级的不行）。&lt;/p&gt;

    &lt;p&gt;这次实验也是首次使用“\0”写入文件，如果使用std::string(char*)构造函数创建会失败，因为这个构造函数还把“\0”作为string’的结尾。如果使用这种构造函数进行写读写数据的初始化，会造成问题。&lt;strong&gt;使用std::string(bug, size)这一个函数代替。&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  <entry>
    <title>6.824:Lab 1 Lock Server</title>
    <link href="http://localhost:4000//2018/05/22/6.824-Lab-1-Lock-Server.html"/>
    <updated>2018-05-22T00:00:00+08:00</updated>
    <id>//2018/05/22/6.824:Lab-1-Lock-Server</id>
    <content type="html">&lt;h1 id=&quot;id-prologue&quot;&gt;Prologue&lt;/h1&gt;
&lt;p&gt;本文是基于MIT 6.824 Distributed Systems 2012年实验，实现一个多服务器的文件系统 YFS(Yet Another File System)。以下是实验指导+报告，实验代码以上传至github，可做交流。&lt;/p&gt;

&lt;h1 id=&quot;id-lab-1lock-server&quot;&gt;Lab 1:Lock Server&lt;/h1&gt;
&lt;h2 id=&quot;id-introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;在这一系列的实验中，你将要实现一个功能齐全的分布式文件系统。为了能够正常的工作，yfs服务器需要一个锁服务来完成文件系统操作。在这个实验中，将要完成一个简单的锁服务。&lt;/p&gt;

&lt;p&gt;锁服务的核心逻辑由两个模块组成，锁服务客户端和服务端。他们之间通过RPC（Remote procedure call 远程调用）进行通信。客户端通过发送acquire请求给锁服务器获取一个锁。锁服务保证每个锁每次只发送给同一个客户端。当客户端完成临界资源区操作后，发送release请求给服务器，释放该锁。此时服务器再将此锁给其他的客户端。&lt;/p&gt;

&lt;p&gt;另外为了实现锁服务，需要RPC保证每一次请求至多传一次（at-more-once）来消除RPC的重复传输。重复传输之所以会存在是因为RPC系统必须重传丢失的RPC报文，如果原先的请求没有丢弃，这种重传会导致RPC重复传递。&lt;/p&gt;

&lt;p&gt;如果重复的RPC被传递，并且处理不当的话，经常会使应用出现问题。比如当A客户发送acquire给服务器索要锁x，服务器给了A，当A通过release释放这个锁的时候，原先acquire请求才到达服务器，服务器又把这个锁给了A，但是A永远不会释放这个锁了（因为他没有acquire这个锁）。这种行为明显是有问题的。&lt;/p&gt;

&lt;h2 id=&quot;id-your-job&quot;&gt;Your Job&lt;/h2&gt;
&lt;p&gt;首要任务就是在一个网络状态良好的情况下实现一个功能正确的锁服务器。功能正确即：在任何一个时间点，一个锁至多有一个用户持有。&lt;/p&gt;

&lt;p&gt;实验将使用lock_tester进行测试以检验锁服务的正确性。比如检验服务器是否在任意时间一个锁至多分配给一个用户。&lt;/p&gt;

&lt;p&gt;第二个任务是去报RPC服务每次请求至多请求一次（at-most-once，以后为了简便使用at-most-once）你可以设置环境变量RPC_LOSSY请求来模拟网络数据丢失的情况。需要修改的文件为rpc.{cc,h}, lock_client.{cc,h}, lock_server.{cc,h}, lock_smain.{cc,h}。&lt;/p&gt;

&lt;p&gt;在这一实验中，你不用考虑服务器或者客户端的失败或者失效，但是需要注意一些有问题的应用。&lt;/p&gt;

&lt;h2 id=&quot;id-detailed-guidance&quot;&gt;Detailed Guidance&lt;/h2&gt;

&lt;p&gt;原则上你可以使用任何的方法来保证在“your job”提到的任务，并通过测试工具的测试。但你可以参考Detailed Guidance中的提示更方便的完成。&lt;/p&gt;

&lt;h3 id=&quot;id-step-one-在网络状态良好的情况下实现锁服务器&quot;&gt;Step One 在网络状态良好的情况下实现锁服务器&lt;/h3&gt;

&lt;p&gt;首先需要在网络状态良好的情况下实现锁服务器，不用考虑重复传输的RPC。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;使用RPC系统 &lt;br /&gt;
  RPC的源代码在rpc的子目录下，一个服务通过申请端口监听方式实现一个RPC对象的方式（rpcs）使用RPC。客户端通过创建客户端对象（rpcc）链接服务端的IP和端口发起RPC。&lt;/p&gt;

    &lt;p&gt;每一个RPC通过有一个唯一的辨识ID。通过在lock_protocol.h中对acquire和release的定义了一个RPC辨识ID，如果需要注册其他的RPC，同样也需要注册（见lock_main.cc）。&lt;/p&gt;

    &lt;p&gt;你可以通过实现lock_client和lock_server的方式学习RPC系统。RPC的请求参数从1-6个不等，返回一个整数作为状态码。返回0就是成功，返回其他的非0正整数就是存在着各种问题，比如超时等等。&lt;/p&gt;

    &lt;p&gt;RPC系统将传入参数序列化为一个stream通过网络传输到另一端，另一端通过反序列化的方式将传入参数还原出来。需要注意的时，RPC反序列化时是不会检查传输信息的类型的。比如你传入一个uint32_t的类型，但是另一端需要把它反序列解成uint64_t，这是可以的。不过会出现各种难以预计的错误。所以调用端和接收端对于参数要统一。&lt;/p&gt;

    &lt;p&gt;RPC库提供了一些常用的C++对象的序列化方法，如：std::string, int 和 char（见rpc.cc）。如果你的RPC调用了不同类型的对象作为传参，那么你就需要自己实现它的序列和反序列的方法。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;实现锁服务  &lt;br /&gt;
  锁服务可以处理不同的锁。每一个锁有一个唯一的标识符lock_protocol::lockid_t。如果一个客户请求这个锁没有在锁服务的集合中，那就创建这个锁，并给这个客户。如果有这个锁，那么检查此锁的状态，如果这个锁被别的客户所使用，那么就让此客户等待，直到占有这个锁的客户将此锁归还。&lt;/p&gt;

    &lt;p&gt;需要修改lock_server.{cc,h}这个文件接收来自客户端的acquire和release请求。并记录每一个锁的状态。&lt;/p&gt;

    &lt;p&gt;一个锁有两种不同的状态&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;free：这个锁没有任何一个客户持有&lt;/li&gt;
      &lt;li&gt;locked：某个客户端正在持有此锁&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;服务器端的锁管理可以使用C++ STL中的 std::map来实现每一个锁状态的保管&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;实现锁客户端&lt;br /&gt;
  客户类lock_client时lock_server的客户端侧的接口。这个接口提供两个方法acquire和release接收和发送RPC接口。可以参考lock_dome.cc这个文件来参考接口的使用方法。&lt;strong&gt;注意lock_client::acquire必须要等到请求到锁后才能返回&lt;/strong&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;处理多线程并发  &lt;br /&gt;
  lock_client和lock_ser都使用了多线程并发的策略。在服务端，RPC提供了一个线程池，每次使用一个空闲的线程。在客户端一侧，不同的线程也可以并发地调用acquire和release。&lt;/p&gt;

    &lt;p&gt;应当使用线程互斥来保证共享数据的安全性。你应该使用线程条件变量以保证锁服务请求中等待逻辑。（可以参考pthread_cond_t的使用方法）&lt;/p&gt;

    &lt;p&gt;线程在一个循环中等待，判断是否可换新。这样可以保护线程在pthread_cond_wait和pthread_cond_timewait()函数唤醒。&lt;/p&gt;

    &lt;p&gt;一个简单的互斥使用策略是，对于lock_server使用一个单一的mutex来保护临界区（那个锁状态的map），使用较粗粒度的互斥锁就可以了。&lt;/p&gt;

    &lt;p&gt;逻辑如图[1]所示:&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/images/distribution_pic/lock_server.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;图[1]: clientA向服务器端请求，获取锁。clientB在向服务器请求后，由于A占用着锁，所以需要等待，直到A向服务器释放此锁&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;lock_protocol::status
lock_server::acquire_lock(int clt, lock_protocol::lockid_t lid, int &amp;amp;r)
{
	lock_protocol::status ret = lock_protocol::OK;
	pthread_mutex_lock(&amp;amp;pcontrol_.pmutex);
	if (lock_map_.count(lid) &amp;lt;= 0) {
		lock_map_[lid].is_locked = true;
	} else {
		while (lock_map_[lid].is_locked == true) {
			pthread_cond_wait(&amp;amp;lock_map_[lid].pcond, &amp;amp;pcontrol_.pmutex);
		}
		lock_map_[lid].is_locked = true;
	}
	ret = lock_protocol::OK;
	pthread_mutex_unlock(&amp;amp;pcontrol_.pmutex);
	return ret;
}

lock_protocol::status
lock_server::release_lock(int clt, lock_protocol::lockid_t lid, int &amp;amp;r) {
	lock_protocol::status ret = lock_protocol::OK;
	pthread_mutex_lock(&amp;amp;pcontrol_.pmutex);
	if (lock_map_.count(lid) &amp;lt;= 0){
		pthread_mutex_unlock(&amp;amp;pcontrol_.pmutex);
		ret = lock_protocol::IOERR;
	}
	if (lock_map_[lid].is_locked) {
		lock_map_[lid].is_locked = false;
		pthread_cond_signal(&amp;amp;lock_map_[lid].pcond);
	} else {
		std::cout &amp;lt;&amp;lt; &quot;lid:&quot; &amp;lt;&amp;lt; lid &amp;lt;&amp;lt; &quot;is not locked, why release?&quot; &amp;lt;&amp;lt; std::endl;
	}
	pthread_mutex_unlock(&amp;amp;pcontrol_.pmutex);
	return ret;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;struct CondLock {
	CondLock() {
		is_locked = false;
		pthread_cond_init(&amp;amp;pcond, NULL);
	}
	virtual ~CondLock() {
		pthread_cond_destroy(&amp;amp;pcond);
	}
	bool is_locked;
	pthread_cond_t pcond;
};
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;id-setp-tworpc实现at-more-once传输&quot;&gt;Setp Two:RPC实现at-more-once传输&lt;/h3&gt;
&lt;p&gt;RPC代码已经完成了客户端的at-most-once：客户端在等待回复超时时，重新发送请求，完成了每次请求时at-most-once服务端所需的信息。但是服务端的at-most-once代码是有部分缺失的。实现的逻辑在rpcs:chechduplicate_and_update和rpcs::add_reply。任务就是完成这两个函数。&lt;/p&gt;

&lt;p&gt;在你的锁服务已经沟通过了lock_tester之前，开启模拟网络丢失的全局变量“export RPC_LOSSY=5”，这时再使用lock_tester会发现很大概率无法成功完成。&lt;/p&gt;

&lt;p&gt;在处理这个实验室，先大体上了解RPC的框架结构。rpc.cc已经包含了一些处理重复请求的代码，你的任务就是补全这些代码。&lt;/p&gt;

&lt;p&gt;rpcc类时时RPC的客户端类，核心代码在call1这个函数中，他序列化RPC请求并传输给RPC服务器，call1在序列化请求时会填充一些RPC的信息：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// add RPC fields before the RPC request data
   req_header h(ca.xid, proc, clt_nonce_, srv_nonce_, xid_rep_window_.front());
   req.pack_req_header(h);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;每一个 req_headre的目的是什么，在call1完成RPC请求的准备后，会在一个while（1）的循环中，等待超时后重传。&lt;/p&gt;

&lt;p&gt;rpcs类管理服务端RPC请求，当收到一个RPC的请求后，调用got_pdu将这个请求分配给线程池中的一个线程。线程池包含了几个固定的线程数。头部的header结构包含了足够的信息来消除所有的重复请求。&lt;/p&gt;

&lt;p&gt;怎样确保at-most-once传输？一个方法就是让服务端记住每一个接收的RPC，每一个RPC有一个唯一的标识符xid（每一个客户保持唯一），和clt_nonce（所有客户保持唯一）。服务端同样要记住每一个RPC返回的值。这种方法保证了at-most-once，但是这么做内存会随着RPC的id和回复不断增长。有一种很好的替换方式就是RPC使用滑动窗口的策略记录xid，要求客户端严格按照指定的顺序来生成xid，比如0，1，2，3.。。这样RPC如果安全传到，就忘掉这个xid，因为xid是按顺序递增，不会有相同的重复，低的就可以忽略。
首先&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;检查每一个请求如果是重复的，就返回记录他的请求&lt;/li&gt;
  &lt;li&gt;如果不是重复的请求就将这个请求记住&lt;/li&gt;
  &lt;li&gt;缩小已经被记住的请求并回复。
第二，如果一个RPC调用服务器端已经完成，可以使用reply_window_来记录已经RPC的返回值。
完成这两步就可使用RPC_LOSSY进行测试。&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;rpcs::rpcstate_t 
rpcs::checkduplicate_and_update(unsigned int clt_nonce, unsigned int xid,
		unsigned int xid_rep, char **b, int *sz)
{
	ScopedLock rwl(&amp;amp;reply_window_m_);

        // You fill this in for Lab 1.

	std::list&amp;lt;reply_t&amp;gt; :: iterator lit = reply_window_[clt_nonce].begin();
	while(lit != reply_window_[clt_nonce].end()) {
		if (lit-&amp;gt;xid == xid) {
			if (lit-&amp;gt;cb_present == false) {
				return INPROGRESS;
			} else {
				*b = lit-&amp;gt;buf;
				*sz = lit-&amp;gt;sz;
				return DONE;
			}
		}
		if (lit-&amp;gt;xid &amp;lt;= xid_rep &amp;amp;&amp;amp; lit-&amp;gt;cb_present) {
			free(lit-&amp;gt;buf);
			lit = reply_window_[clt_nonce].erase(lit);
			continue;
		}

		lit++;
	}

	if (reply_window_[clt_nonce].size() &amp;gt; 0){
		if (reply_window_[clt_nonce].back().xid &amp;gt; xid)
			return FORGOTTEN;
	}

	reply_t now_reply(xid);
	lit = reply_window_[clt_nonce].begin();
	while(lit != reply_window_[clt_nonce].end()){
		if (lit-&amp;gt;xid &amp;lt; xid)
			break;
		lit++;
	}
	if (lit != reply_window_[clt_nonce].begin())
		lit--;
	reply_window_[clt_nonce].insert(lit, now_reply);
	return NEW;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;void
rpcs::add_reply(unsigned int clt_nonce, unsigned int xid,
		char *b, int sz)
{
	ScopedLock rwl(&amp;amp;reply_window_m_);
        // You fill this in for Lab 1.
	std::list&amp;lt;reply_t&amp;gt; :: iterator lit = reply_window_[clt_nonce].begin();
	for (; lit != reply_window_[clt_nonce].end(); lit++) {
		if (lit-&amp;gt;xid == xid) {
			lit-&amp;gt;buf = b;
			lit-&amp;gt;sz = sz;
			lit-&amp;gt;cb_present = true;
			break;
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;
</content>
  </entry>
  

</feed>
