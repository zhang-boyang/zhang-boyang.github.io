<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
        <link rel="alternate" type="application/rss+xml" href="/atom.xml" />
        <link rel="shortcut icon" type="image/png" href="/images/icon.jpg" />
        <link href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet">
        <link rel="stylesheet" href="/assets/css/bootstrap.min.css">
        <link rel="stylesheet" href="/assets/css/share.min.css">
        <link rel="stylesheet" href="/assets/css/highlight/github-gist.css">
        <script src="/assets/js/jquery.min.js"></script>
        <script src="/assets/js/jquery.share.min.js"></script>
        <script src="/assets/js/highlight.pack.js"></script>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
        <script> hljs.initHighlightingOnLoad(); </script>
        
        <title>6.824:Lab4-5 Cache | Note of John Brown</title>
        
    </head>
    <body>
        <div class="container-fluid">
            <div class="row row-header">
                <div class="col-md-6 col-md-offset-3">
                    <ul class="list-inline list-nav">
                        <li><a href="/"><b>Note of John Brown</b></a></li>
                        <li><a href="/archive">archive</a></li>
                        <li><a href="/about">about</a></li>
                        <li><a href="/atom.xml">atom</a></li>
                    </ul>
                </div>
            </div>

            <div class="row">
                <div class="col-xs-12 col-sm-12 col-md-6 col-md-offset-3 col-main">
                    <div class="content">
    <h1>6.824:Lab4-5 Cache</h1>

    <h1 id="id-lab4-5-caching-locks-and-extents">Lab4-5 Caching Locks and Extents</h1>
<h2 id="id-introductionlab4">Introduction(Lab4)</h2>
<p>在这两个实验中，主要建立了锁和存储服务的缓存，以减少服务器负载并提高客户端的性能。</p>

<p>例如在Lab3中的测试，在一个YFS文件夹中建立100个文件，需要像这个文件夹（directory）的锁请求100次。这次的实验就是修改锁服务，让锁客户端只需要发送一次acquire RPC，把这个锁保存在缓存中，直到有其他的yfs_client需要再释放。</p>

<p>这次的挑战需要修改客户端和服务器端的协议。例如当client2 需要某个被client1缓存在其本地的锁，需要服务通过revoke RPC revoke（找不出比较合适的词翻译） client1 的那个锁，返还给server。client2 才能够得到。</p>

<h2 id="id-getting-startedlab4">Getting Started(Lab4)</h2>
<p>需要改动的文件有：</p>
<ul>
  <li>lock_client_cache.{cc,h}:这两个文件替换以前的lock_client文件，主要实现客户端缓存。</li>
  <li>lock_server_cache.{cc,h}:同样的，代替以前的lock_server文件，实现对应的锁缓存服务</li>
  <li>handle.{cc,h}:这个类主要包含了缓存节点之间的通信RPC，使用revoke和retry时使用。</li>
  <li>tprintf.h:这个文件包含了一个宏，用来打印很多debug的信息，尤其是在分布式锁的调试中起到了很关键的作用。</li>
</ul>

<h2 id="id-step-one-design-the-protocollab4">Step One: Design the Protocol(Lab4)</h2>

<p>锁客户端需要对每一个锁记录其状态，并要有一个协议来表示他们目前的状态。设计这套协议，并思考协议怎样促使这些状态转换。
这是推荐的一套客户端协议：</p>
<ul>
  <li>none      ：客户端不知道这个锁的存在</li>
  <li>free      ：客户端拥有此锁并且也没有线程使用此锁</li>
  <li>locked    ：客户端拥有此锁，并且有线程在使用此锁</li>
  <li>aquiring  ：客户端正在请求此锁</li>
  <li>releasing ：客户端正在释放此锁</li>
</ul>

<p>状态转移图如图[1]所示:</p>

<p><img src="/images/distribution_pic/state_machine.png" alt="image" /></p>

<p><em>图[1] 整体系统结构。灰色圆圈中是每一个锁的状态。箭头上的时状态转移的条件。比如acq请求，release释放，revoke，retry 的远程调用等</em></p>

<p>为了满足这个记录客户端的这些信息，每个锁的结构需要重新规划。如下所示：</p>
<pre><code>class ClientLock{
  public:
	pthread_cond_t wait_acq_;
	pthread_cond_t wait_free_;
	bool is_retried;
	bool is_revoked;
	bool is_finished;
	AcqRet::lock_status status;
	ClientLock():
	    is_retried(false),
	    is_revoked(false),
	    is_finished(false),
	    status(AcqRet::NONE){
	    pthread_cond_init(&amp;wait_acq_, NULL);
	    pthread_cond_init(&amp;wait_free_, NULL);
	}
};
class AcqRet {
public:
	enum status {OK = 0, RETRY};
	enum lock_status {NONE, FREE, LOCK, RELEASING, ACQING};
};

lock_protocol::status
lock_client_cache::acquire(lock_protocol::lockid_t lid)
{
  int ret = lock_protocol::OK;
  ScopeLock map_lock(&amp;lock_map_mutex_);
  ClientLock &amp;lock_item = m_lock_map_[lid];
  bool  is_used = false;
  while(!is_used){
    tprintf("lock_item.status is %d\n", lock_item.status);
	switch(lock_item.status)
	{
	  case AcqRet::NONE:
	      DealWithStatusNONE(lock_item, lid);
		  break;
	  case AcqRet::FREE:
	      DealWithStatusFREE(lock_item, lid);
	      is_used = true;
		  break;
	  case AcqRet::LOCK:
	      DealWithStatusLOCK(lock_item, lid);
		  break;
	  case AcqRet::ACQING:
	      DealWithStatusACQ(lock_item, lid);
		  break;
	  default:
		VERIFY(false);
	}
  }
  return ret;
}

void lock_client_cache::DealRelease(ClientLock &amp; lock_item, lock_protocol::lockid_t lid)
{
    if (lock_item.status == AcqRet::NONE) return;
    if (lock_item.is_revoked){
        int r;
        lock_item.status = AcqRet::NONE;
        lock_item.is_finished = false;
        lock_item.is_revoked = false;
        pthread_mutex_unlock(&amp;lock_map_mutex_);
        lu-&gt;dorelease(lid);
        lock_protocol::status ret = cl-&gt;call(lock_protocol::release, lid, id, r);
        tprintf("I send it %llu back to server\n", lid);
        VERIFY(ret == lock_protocol::OK);
        pthread_mutex_lock(&amp;lock_map_mutex_);
    }else{
        lock_item.status = AcqRet::FREE;
        lock_item.is_finished = true;
    }
    pthread_cond_broadcast(&amp;lock_item.wait_free_);
    pthread_cond_broadcast(&amp;lock_item.wait_acq_);
}
void lock_client_cache::DealWithStatusACQ(ClientLock &amp; lock_item, lock_protocol::lockid_t lid)
{
    if (lock_item.is_retried){
        SendAcqToSvr(lock_item, lid);
        tprintf("I am here retried 1\n");
    }
    while(lock_item.status == AcqRet::ACQING &amp;&amp; !lock_item.is_retried){
        tprintf("I am waiting here\n");
        pthread_cond_wait(&amp;lock_item.wait_acq_, &amp;lock_map_mutex_);
    }
    if (lock_item.is_retried &amp;&amp; lock_item.status == AcqRet::ACQING){
        SendAcqToSvr(lock_item, lid);
        tprintf("I am here retried 2\n");
    }
}
void lock_client_cache::DealWithStatusFREE(ClientLock &amp; lock_item, lock_protocol::lockid_t lid)
{
    lock_item.is_finished = false;
     lock_item.status = AcqRet::LOCK;
}
void lock_client_cache::DealWithStatusLOCK(ClientLock &amp; lock_item, lock_protocol::lockid_t lid)
{
    while(lock_item.status == AcqRet::LOCK){
        pthread_cond_wait(&amp;lock_item.wait_free_, &amp;lock_map_mutex_);
    }
}
void lock_client_cache::DealWithStatusNONE(ClientLock &amp; lock_item, lock_protocol::lockid_t lid)
{

    SendAcqToSvr(lock_item, lid);
}

int lock_client_cache::SendAcqToSvr(ClientLock &amp; lock_item, lock_protocol::lockid_t lid){
    int r;
    lock_item.is_retried  = false;
    pthread_mutex_unlock(&amp;lock_map_mutex_);
    int ret = cl-&gt;call(lock_protocol::acquire, lid, id, r);
    pthread_mutex_lock(&amp;lock_map_mutex_);
    tprintf("I am send a msg to svr, ret=%d\n", ret);
    if (ret == AcqRet::OK){
        lock_item.status = AcqRet::FREE;
    } else if (ret) {
        lock_item.status = AcqRet::ACQING;
    } else {
        tprintf("unknow svr return %d\n", ret);
    }
    return ret;

}
</code></pre>
<p>一个单独的客户端可能有多个线程等待相同的锁，但是每个客户端只有一个线程需要与server互动（interacting）。一旦某个有锁的线程释放掉此锁，那么就会唤醒那些等待这个所的其余线程。在等待的这些锁中，会有一个获得此锁。</p>

<p>当一个client通过acquire RPC请求server时，如果此锁没有被任何client占有时，server返回 OK ，如果此锁被某个client占有时，返回RETRY。同时，server发送一个revoke给占有此锁的client，让他交出这个锁。当这个锁归还给server时，再向请求这个锁的client发出retry RPC，让它重新请求这个锁。如下图所示：</p>

<p>一旦某个client获得了这个锁，client将会缓存此锁（cache，占有）。当他使用晚此锁时，不用release给server，如果这个client再需要acquire此锁时，就不需要向server请求了，从而减少了网络请求，提高了效率。知道有其他的client进行锁请求时，此client才（并且必须）交还此锁给server。</p>

<p>服务器端应该思考怎样保存锁信息，出了是否locked这种信息，还需要保存比如每一个锁现在被谁占（cached），哪些clients正在等待此锁的释放等。</p>

<p>服务端处理如图[2]所示:</p>

<p><img src="/images/distribution_pic/lock_cache.png" alt="image" /></p>

<p><em>图[2] 黑色为锁服务器在有缓存下的处理方式，红蓝为客户端，此图模拟了当锁遇到竞争时的状况，A先得锁，B等锁释放</em></p>

<pre><code>struct CondLockCache : public CondLock{
	CondLockCache() {
	}
	~CondLockCache() {
	}
	std::string user_id_;       //lab4 add
};

//record which clints are waiting for this lock releasing
std::map&lt;lock_protocol::lockid_t, std::queue&lt;std::string&gt; &gt; m_retry_queue_; 

int lock_server_cache::acquire(lock_protocol::lockid_t lid, std::string id, 
                               int &amp;r)
{
      AcqRet::status ret;
	  ScopeLock map_lock(&amp;m_lock_mutex_);
	  CondLockCache&amp; lock_item =  m_map_lock_[lid];
	  if (lock_item.is_locked == false){
		  lock_item.is_locked = true;
		  lock_item.user_id_ = id;
		  ret = AcqRet::OK;
	  }else{
		  VERIFY (!lock_item.user_id_.empty());
		  pthread_mutex_lock(&amp;m_retry_mutex_);
		  m_retry_queue_[lid].push(id);
		  pthread_mutex_unlock(&amp;m_retry_mutex_);
		  //ConnectToClient conn_c(lock_item.user_id_);
		  ret = AcqRet::RETRY;
		  pthread_mutex_unlock(&amp;m_lock_mutex_);
		  int ret_r;
		  ret_r = handle(lock_item.user_id_).safebind()-&gt;call(rlock_protocol::revoke, lid, r);
		  if (ret_r != rlock_protocol::OK) tprintf("rlock_protocol::revoke failed!!\n");
	  }
  return ret;
}
</code></pre>
<p>在锁服务的acquire大致逻辑是，先判断此锁是否被锁，如果没有，直接返回OK，如果有，记录此客户端ID（IP：PORT）放入retry等待队列。随后像拥有此锁的客户端发送revoke，让其归还此锁。</p>
<pre><code>lock_protocol::status
lock_server_cache::release(lock_protocol::lockid_t lid, std::string id, 
         int &amp;r)
{
  tprintf("%s send %llu to server\n", id.c_str(), lid);
  lock_protocol::status ret = lock_protocol::OK;
  ScopeLock map_lock(&amp;m_lock_mutex_);
  if (m_map_lock_.count(lid) &lt;= 0){
	  tprintf("lid is not acquired by anyone, why do you release?\n");
  }
  CondLockCache&amp; lock_item =  m_map_lock_[lid];
  lock_item.is_locked = false;
  ScopeLock retry_lock(&amp;m_retry_mutex_);
  std::queue&lt;std::string&gt; &amp;rty_q = m_retry_queue_[lid];
  tprintf("now the retry queue size is %zu\n", rty_q.size());
  if (!rty_q.empty()){
      int ret_r;
	  std::string user_id = rty_q.front();
	  rty_q.pop();
	  //ConnectToClient conn_c(user_id);
	  pthread_mutex_unlock(&amp;m_lock_mutex_);
	  pthread_mutex_unlock(&amp;m_retry_mutex_);
	  ret_r = handle(user_id).safebind()-&gt;call(rlock_protocol::retry, lid, r);
	  if (ret_r != rlock_protocol::OK) tprintf("rlock_protocol::retry failed!!\n");
	  pthread_mutex_lock(&amp;m_lock_mutex_);
	  pthread_mutex_lock(&amp;m_retry_mutex_);
	  if(!rty_q.empty()){
	    pthread_mutex_unlock(&amp;m_lock_mutex_);
        pthread_mutex_unlock(&amp;m_retry_mutex_);
        ret_r = handle(user_id).safebind()-&gt;call(rlock_protocol::revoke, lid, r);
        if (ret_r != rlock_protocol::OK) tprintf("rlock_protocol::revoke failed!!\n");
        pthread_mutex_lock(&amp;m_lock_mutex_);
        pthread_mutex_lock(&amp;m_retry_mutex_);
	  }
  }
  return ret;
}
</code></pre>
<p>还锁时，查看等待队列，如果有等待发送retry给等待Client_X。这里需要注意：⚠️<strong>如果等待队列里还有等待此锁的客户端，要事先发送revoke给 Client_X，虽然它还没有拿到此锁，因为如果不这样，就再没有客户会请求此锁，也不会有revoke发送给Client_X</strong></p>

<p>这里的user_id 直接记录了当前锁持有client的IP和port 同样在m_retry_queue_这个map中 queue中的string也是IP：PORT的形式，使用这样的方式第一能够保证唯一性，第二方便。
提示：当发送RPC时，要释放目前占有的锁，一个RPC可能维持的时间较长，如果你不希望其他的线程跟着一起等的话，就将它们释放掉。而且不释放还会导致分布式死锁。</p>

<p>下列两个问题能够帮助你思考和设计：</p>
<ul>
  <li>当一个client的线程保持一个锁，另一个线程发出acquire请求时，将会发生什么？这时时不会发送rpc请求的。</li>
  <li>当一个客户端持有锁时，revoke请求应当怎样处理？当一个client在收到acquire的反馈之前收到了retry，该怎么办？</li>
</ul>

<p>提示：当一个client在收到acquire的反馈之前收到了retry，client应当记下这个请求，如果忽视了这个retry，如果你的acquire收到的是RETRY的话，client将永远陷入等待之中。（因为server不会再发送retry）</p>
<ul>
  <li>当一个client在收到acquire的反馈之前收到了revoke，该怎么办？记录起来（跟记录retry一样），返回OK，使用完这个锁后立即归还给server，不要做缓存。（因为server同样不会再发送revoke，此client以为没有其他clients需要，一直不肯归还，其他clients一直饥饿）</li>
</ul>

<h2 id="id-introductionlab5">Introduction(Lab5)</h2>
<p>接下来要做的是对extents，也就是存储数据的服务实现缓存。原因与第一个锁缓存服务一样，都是为了减小开销，增加性能。最主要的任务就是要确保extents缓存下，某个client最终处理的某个数据，是上一个client在处理完后的结果。（不能在处理相同的数据时，各处理各的数据）。</p>

<p>首先需要在client端添加一个本地的extent存储做cache。extent 客户端将再这个基础上对cache进行操作，同时客户端只有在没有某个extent并获取extent或者属性（attr）时访问extent服务器，并在其他extent客户端需要访问修改的时候将脏数据回写入extent服务器。与锁缓存服务有异曲同工之妙！</p>

<h2 id="id-step-oneextent-cachelab5">Step One:Extent Cache(Lab5)</h2>
<p>在第一步中，需要做一个带缓存的extent 客户端，不用考虑数据的一致性。首先新的extent_client::get应该检查客户端是否已经在本地的cache中，如果不在，访问extent服务器，将数据取回来，缓存在自己的数据库中。如果在，直接返回。put()函数中，直接替换cached中的数据，没有必要将已经修改的数据（脏数据）传给服务器，保存在缓存中。remove()应该删除本地的extent服务器。同样extent客户端应当记录一个数据的属性。</p>
<pre><code>class FileData {
 public:
    FileContent file_content_;
    bool is_present;
    bool is_dirty;

    FileData():
        is_present(false),
        is_dirty(false)
    {}
};
</code></pre>
<p>这是extent_client中extent的数据结构，除了以前的file content结构之外，添加了<code>is_present</code>,<code>is_dirty</code>两个标记。一个表示是否在本地cache中，一个表示已经修改，需要将脏数据回写入extent服务器。</p>
<h2 id="id-step-twocache-consistencylab5">Step Two:Cache Consistency(Lab5)</h2>
<p>在第二步中，要确保每一个get()请求请求到的数据时最近的put()改动过的数据，即使get与put的调用方时两个不同的extent客户端。extent客户端和锁服务相互合作以确保每一个inum（indoe）中的数据是一致的。当你释放锁的时候，要flush一下extent对应的inum数据，即回写入extent脏数据以更新extent服务器数据。（如果remove了，直接删了extent服务中对应的数据就好）</p>

<p>假设clientA请求了锁inum，从extent服务器上get了对应的文件数据并在客户端自己的cache中修改了数据，当前clientA即缓存着锁服务的锁，也缓存着extent服务的数据。那么当clientB想要访问这份数据时，首先要拿到client缓存的锁，所以clientA要现将脏数据写入extent服务器中（返还这份数据），再释放对应的锁。这样clientB就能够看到clientA修改完后的数据了。</p>

<p>这里还需要注意使用锁的两个目的：</p>
<ol>
  <li>确保文件系统中的每一个操作的原子性</li>
  <li>完成了extent缓存的一致性</li>
</ol>

<p>确保每一个yfs_client对extent的操作包含在acquire和release请求中。</p>

<p>YFS服务应当在释放锁到锁服务器上的时候再调用flush，而不是释放锁的时候。不要每一次调用lock_client::release()flush，而是要在lock_client调用<code>cl-&gt;call(lock_protocol::release, lid, id, r);</code>即真正返还锁给锁服务器时（锁服务向该client调用revoke时）再调用。否则就有些矫枉过正了。</p>

<p>在这里，实验环境提供了lock_release_user类，这是一个虚类仅仅支持dorelease的成员方法。我们需要实现一个通过调用dorelease能够调用flush的子类。</p>

<pre><code>extent_protocol::status
extent_client_cache::get(extent_protocol::extentid_t eid, std::string &amp;buf)
{
  extent_protocol::status ret = extent_protocol::OK;
  if (extent_file_cache_[eid].is_present){
      time_t now = time(0);
      extent_file_cache_[eid].file_content_.first.atime = static_cast&lt;int&gt;(now);
      buf = extent_file_cache_[eid].file_content_.second;
  }else{
    extent_protocol::attr &amp;attr = extent_file_cache_[eid].file_content_.first;
    ret = cl-&gt;call(extent_protocol::get, eid, buf);
    ret = cl-&gt;call(extent_protocol::getattr, eid, attr);
    extent_file_cache_[eid].is_present = true;
    extent_file_cache_[eid].file_content_.second = buf;
  }
  return ret;
}

extent_protocol::status
extent_client_cache::getattr(extent_protocol::extentid_t eid,
               extent_protocol::attr &amp;attr)
{
  extent_protocol::status ret = extent_protocol::OK;
  if (extent_file_cache_[eid].is_present){
      attr = extent_file_cache_[eid].file_content_.first;
  } else {
      ret = cl-&gt;call(extent_protocol::getattr, eid, attr);
  }
  return ret;
}

extent_protocol::status
extent_client_cache::put(extent_protocol::extentid_t eid, std::string buf)
{
  extent_protocol::status ret = extent_protocol::OK;
  time_t now = time(0);

  std::map&lt;uint64_t, FileData&gt;::iterator file_cache_it = extent_file_cache_.find(eid);
  if (file_cache_it == extent_file_cache_.end()){
      extent_file_cache_[eid].file_content_.first.atime = static_cast&lt;int&gt;(now);
  }
  extent_file_cache_[eid].file_content_.first.mtime = static_cast&lt;int&gt;(now);
  extent_file_cache_[eid].file_content_.first.ctime = static_cast&lt;int&gt;(now);
  //int r;
  //ret = cl-&gt;call(extent_protocol::put, eid, buf, r);
  extent_file_cache_[eid].file_content_.first.size = buf.size();
  extent_file_cache_[eid].file_content_.second = buf;
  extent_file_cache_[eid].is_dirty = true;
  extent_file_cache_[eid].is_present = true;
  return ret;
}

extent_protocol::status
extent_client_cache::remove(extent_protocol::extentid_t eid)
{
  extent_protocol::status ret = extent_protocol::OK;
  std::map&lt;uint64_t, FileData&gt;::iterator file_cache_it = extent_file_cache_.find(eid);
  if (file_cache_it != extent_file_cache_.end()){
      extent_file_cache_.erase(file_cache_it);
  }else{
      int r;
      ret = cl-&gt;call(extent_protocol::remove, eid, r);
  }

  return ret;
}

extent_protocol::status extent_client_cache::flush(extent_protocol::extentid_t eid)
{
    extent_protocol::status ret = extent_protocol::OK;
    std::map&lt;uint64_t, FileData&gt;::iterator file_cache_it = extent_file_cache_.find(eid);
    if (file_cache_it == extent_file_cache_.end()){
        int r;
        ret = cl-&gt;call(extent_protocol::remove, eid, r);
        return ret;
    }

    if (file_cache_it-&gt;second.is_dirty){
        int r;
        std::string &amp;buf = file_cache_it-&gt;second.file_content_.second;
        ret = cl-&gt;call(extent_protocol::put, eid, buf, r);
        tprintf("put %d back to server, ret=%d\n", eid, ret);
    }
    extent_file_cache_[eid].is_dirty   = false;
    extent_file_cache_[eid].is_present = false;
    return ret;
}
</code></pre>
<p>在缓存实验后，我们可以打开rpc的记录日志（在环境中设置 <code>
export RPC_COUNT=25</code>
）,它会记录所有rpc请求的数量。在通过执行test-lab-3-c,可以观察在加缓存和没有缓存情况下，客户端请求服务器的次数。实验手册上说如果设计得当，锁服务的请求会降低10倍（很惭愧，只降低了3倍）。其他的请求次数也会有大幅度下降。图[3]为实验对比：</p>

<p><img src="/images/distribution_pic/cache_perform.png" alt="image" /></p>

<p><em>图[3]acq没有试验手册下降的多，release到是下降了10倍左右，在extent的试验中，Put和remove下降最大，Get与GetAttr下降明显。GetInum因为没有做缓存，而且在我做实验中设计的不好，随机生成，不太方便做缓存，所以也就没下降</em></p>

<h2 id="id-hints">Hints</h2>

<ul>
  <li>确保使用线程mutex来保护客户端的extent cache，防止多线程资源竞争。</li>
  <li>如果你对extent中的某个data只是做了只读（read-only）的操作，久不要将这一数据flush入extent服务器了，应该使用一个标记位来记录数据是否被修改（<code>is_dirty</code>）</li>
  <li>这个实现可以不怎么修改yfs_client.cc这一文件，可以通过继承client_extent的方式，实现extent_client_cache，重新定义put get等方式。（因为yfs_client调用extent的方式是指针，这一点实验设计的很巧妙）</li>
</ul>


    
      <p>
        <span class="gray">22 May 2018 by John Brown </span>
      </p>
    
</div>

<div class="social-sharing"></div>

<script type="text/javascript">
$(function(){
    $('.social-sharing').share({
        sites: ['wechat'],
    });
});
</script>

<div class="comments">
    <div id="disqus_thread"></div>
</div>

<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'NoteOfJohnBrown'; // required: replace example with your forum shortname
var disqus_developer = 1;

/* * * DON'T EDIT BELOW THIS LINE * * */
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>

                </div>
            </div>
        </div>
    <link rel="stylesheet" href="/assets/css/blog.css">
    </body>
</html>
