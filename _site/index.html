<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0">
        <link rel="alternate" type="application/rss+xml" href="/atom.xml" />
        <link rel="shortcut icon" type="image/png" href="/images/icon.jpg" />
        <link href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=PT+Serif" rel="stylesheet">
        <link rel="stylesheet" href="/assets/css/bootstrap.min.css">
        <link rel="stylesheet" href="/assets/css/share.min.css">
        <link rel="stylesheet" href="/assets/css/highlight/github-gist.css">
        <script src="/assets/js/jquery.min.js"></script>
        <script src="/assets/js/jquery.share.min.js"></script>
        <script src="/assets/js/highlight.pack.js"></script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
        <script> hljs.initHighlightingOnLoad(); </script>
        
        <title>Note of John Brown</title>
        
    </head>
    <body>
        <div class="container-fluid">
            <div class="row row-header">
                <div class="col-md-6 col-md-offset-3">
                    <ul class="list-inline list-nav">
                        <li><a href="/"><b>Note of John Brown</b></a></li>
                        <li><a href="/archive">archive</a></li>
                        <li><a href="/about">about</a></li>
                        <li><a href="/atom.xml">atom</a></li>
                    </ul>
                </div>
            </div>

            <div class="row">
                <div class="col-xs-12 col-sm-12 col-md-6 col-md-offset-3 col-main">
                    
  <h1><a href="/2019/09/28/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%88%86%E7%89%87%E7%9A%84%E8%89%BA%E6%9C%AF(2017-6.824-Lab4).html">分布式系统分片的艺术(2017-6.824 Lab4)</a></h1>

  <div class="content">

    
      <h2 id="id-为什么要分片">为什么要”分片”</h2> <p>分片即将整体的数据分而治之。以多节点系统工作的方式共同完成一个工作。能够减轻节点服务器压力，提高整个集群节点利用率与性能。</p> <p>这里引申出一个问题，为什么需要使用分片这一方式。在分布式系统中，有主副节点(master replica 数据上，Leader Follower raft一致性协议上)。能否在主副节点同时对数据进行操作，哪怕只是对副本进行读操作。至少在要求<strong>强一致</strong>或者<strong>线性一致的</strong>的存储系统中，答案是否定的。在这些系统中，副本只负责提高分布式系统的可用性，对系统的容灾性进行提升。client不能使用replica中的数据。因为副本节点没有向一致性propose的权利，无法保证这次操作已达成了共识。最主要的原因就是如果集群出现了脑裂,导致线性不一致。</p> <p>这里需要提到一点，就是原始的paxos协议，没有raft的leader和follower的区别。paxos角色为proposer和acceptor，但是任何节点都同时扮演这两种角色，一个基础的paxos协议需要多次网络请求（prepare，accept…）。任何节点都可以进行propose的结果就是导致了非常容易发生冲突，在多请求时很很难达成共识。所以一般在工程实现时使用multi-paxos，即也是选出leader打头阵。</p> <h2...
      <br/>
      <a href="/2019/09/28/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%88%86%E7%89%87%E7%9A%84%E8%89%BA%E6%9C%AF(2017-6.824-Lab4).html">read more</a>
   

    <p>
      <span class="gray">28 Sep 2019 by John Brown</span>
    </p>
  </div>

  <h1><a href="/2019/09/28/Raft%E7%9A%84go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BD%BF%E7%94%A8(2017-6.824-Lab2-Lab3).html">Raft的go语言实现与使用(2017-6.824 Lab2-Lab3)</a></h1>

  <div class="content">

    
      <p>Raft协议的解读已经有很多，整体最主要的协议其实用论文中的两幅图就能大致表述了。最后再加上一个Snapshot的图解析基本就全。相对与Paxos而言，我个人认为Raft从实现协议的角度来说反而比Paxos要复杂。主要就是在于index的处理上。这此使用Go语言实现了一个Raft协议并支持snapshot。并且使用Raft协议实现了一个分布式的kv存储。</p> <h2 id="id-raft的实现">Raft的实现</h2> <h3 id="id-raft的成员变量">Raft的成员变量</h3> <pre><code class="language-go"> type Raft struct...
      <br/>
      <a href="/2019/09/28/Raft%E7%9A%84go%E8%AF%AD%E8%A8%80%E5%AE%9E%E7%8E%B0%E4%B8%8E%E4%BD%BF%E7%94%A8(2017-6.824-Lab2-Lab3).html">read more</a>
   

    <p>
      <span class="gray">28 Sep 2019 by John Brown</span>
    </p>
  </div>

  <h1><a href="/2019/01/29/%E5%85%B3%E4%BA%8EABtest%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%E6%AD%A3%E4%BA%A4%E6%80%A7%E8%AE%A8%E8%AE%BA.html">关于ABtest哈希算法正交性讨论</a></h1>

  <div class="content">

    
      <h1 id="id-关于abtest哈希算法正交性讨论">关于ABtest哈希算法正交性讨论</h1> <h2 id="id-hash正交性">Hash正交性</h2> <p>正交性定义：首先任意一个hash_type都可以均匀的将流量均匀分成100份。使用hash_type1 进行哈希后得到<script type="math/tex">x_i,i\in[0,100)</script>,即每份百分之1的流量，再使用hash_type2进行哈希后得到<script type="math/tex">y_i,i\in[0,100)</script>。如果某个key在被hash_type1进行hash后得到的流量与被hash_type2得到的流量是无关的，那么就说明hash_type1与hash_type2是两中hash方式是正交的。</p> <p>例如一个1000000用户的流量，经过hash_type1与hash_type2进行hash，理想情况下 中具有相同每份y中的相同x流量的个数也是同样多的，即100个。如果使用数组命中计数的话，list[<script...
      <br/>
      <a href="/2019/01/29/%E5%85%B3%E4%BA%8EABtest%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%E6%AD%A3%E4%BA%A4%E6%80%A7%E8%AE%A8%E8%AE%BA.html">read more</a>
   

    <p>
      <span class="gray">29 Jan 2019 by John Brown</span>
    </p>
  </div>

  <h1><a href="/2018/08/20/ThreadPool,-Coroutine,-Promise%E4%B8%8EFuture.html">ThreadPool, Coroutine, Promise与Future</a></h1>

  <div class="content">

    
      <h1 id="id-threadpool-coroutine-promise与future">ThreadPool, Coroutine, Promise与Future</h1> <h2 id="id-threadpool">ThreadPool</h2> <p>线程池，顾名思义，就是放着很多线程的大“池子”。主要用作并发量大，但每个任务需要处理的时间不是很长。比如接受或者发送网络请求的任务。之所以使用线程池，原因就是在线程的传创建和销毁在高并发先开销十分大，如果将线程先创建好放入“池子”中待命，随用随取。降低不必要的开销十分划算。</p> <p>实现也非常的简单，创建队列，使用生产消费者模型（向队列中增加任务即生产者，将任务执行完成即消费者）。主要实现逻辑如下：</p> <pre><code class="language-c++">ThreadPool::ThreadPool(int...
      <br/>
      <a href="/2018/08/20/ThreadPool,-Coroutine,-Promise%E4%B8%8EFuture.html">read more</a>
   

    <p>
      <span class="gray">20 Aug 2018 by John Brown</span>
    </p>
  </div>

  <h1><a href="/2018/07/25/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%93%88%E5%B8%8C%E5%88%86%E5%8C%BA%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98.html">分布式系统哈希分区处理问题</a></h1>

  <div class="content">

    
      <h1 id="id-分布式系统哈希分区处理问题">分布式系统哈希分区处理问题</h1> <p>分布式系统的存在，其重要的一个原因就是为了能够将负载很好的均摊到各个节点上，以数量优势提高性能，即分布式系统的可扩展性。大数据可以分布在多个主机的磁盘上，查询也可以有多个主机分别进行处理。分区的实现方式有很多种，目标基本一致，更好的将负载和查询均匀的分布在各个节点。如果分区不均匀(skew)，那么会使此分区甚至整个系统的效率下降。这篇文章主要讨论在高负载下如何避免热点(hot spot)数据。</p> <p>本文主要针对我工作中所接触的RTRec流式系统中所使用的分布式系统hash分区做讨论，并从架构的美学(aesthetics)提出一个相对简约分区处理系统。现有流式集群使用一致性哈希，主要有两个原因：</p> <ul> <li>保证其负载均衡。这里指的是数据层面的负载均衡，因为接入层已经进行了负载均衡。</li> <li>保证处理与查询的key落在同一台机器(节点)上。流式系统一个重要的工作就是收集客户端产生的上报数据做处理。如果要对某一个用户的行为进行上报，最高效的方法就是将这个用户的行为进行收集缓存，统计，入库。这些要求就要保证处理同一个用户的机器是相同的。同理，对APP信息的曝光，下载记录也需要同一key落在同一台机器上。</li> </ul> <p>系统由三部分组成，zookeeper，WatchDog，节点集群。拓扑图如图[1]所示。节点之间的通讯通过一个全局的路由表作为路由，节点集群中的每个节点与zk相连。zk有两个作用，第一个作用是保持路由表的一致性，第二个作用监控集群中的每一个节点的状态。WatchDog的作用是针对现有节点集群做一致性哈希策略，即生成全局的路由表，将路由表放在zk上进行保管，一旦路由表有变化，通知集群中的各个节点去zk上更新最新的路由表。</p>...
      <br/>
      <a href="/2018/07/25/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%93%88%E5%B8%8C%E5%88%86%E5%8C%BA%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98.html">read more</a>
   

    <p>
      <span class="gray">25 Jul 2018 by John Brown</span>
    </p>
  </div>

  <h1><a href="/2018/05/22/6.824-Lab6-7-Replicated-State-Machine.html">6.824:Lab6-7 Replicated State Machine</a></h1>

  <div class="content">

    
      <h1 id="id-paxos">Paxos</h1> <h2 id="id-introduction">Introduction</h2> <p>在实验6-7中，将使用RSM（replicated state machine）的方法复写锁服务，在这个方法中，有一个节点为master，master接收来自client节点的请求并在所有的节点以相同的方式在所有的副本下执行。</p> <p>当master节点失败，任何一个副本都会接管master的工作，因为这些副本都与那个失败的节点有着相同的状态。其中一个比较有挑战性任务时确保集群中的每一个节点所接受的信息是一致的（即哪些是副本，谁是master，哪些副本是keepalive的），即使存在网络分裂（network partition）或者是乱序的情况，数据仍然要保持一致。这里我们使用Paxos协议来实现这一策略。</p> <p>在此次实验中，将要实现Paxos并使用达成集群成员的改变（view...
      <br/>
      <a href="/2018/05/22/6.824-Lab6-7-Replicated-State-Machine.html">read more</a>
   

    <p>
      <span class="gray">22 May 2018 by John Brown</span>
    </p>
  </div>


<br/>


<div class="pagination">
  
    <span>&laquo; Prev</span>
  

  
    
      <em>1</em>
      
    
    
      <a href="/page2">2</a>
    
  

  
    <a href="/page2">Next &raquo;</a>
  
</div>

                </div>
            </div>
        </div>
    <link rel="stylesheet" href="/assets/css/blog.css">
    </body>
</html>
